"""
çº¿æ€§å›å½’è®­ç»ƒé—®é¢˜è¯Šæ–­è„šæœ¬
å¸®åŠ©ç†è§£ä¸ºä»€ä¹ˆæ¨¡å‹è®­ç»ƒä¸ç¨³å®šå’Œè¯¯å·®å¤§çš„é—®é¢˜
"""

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

print("ğŸ” çº¿æ€§å›å½’é—®é¢˜è¯Šæ–­")
print("=" * 50)

# 1. æ¼”ç¤ºé”™è¯¯çš„æ•°æ®æ ¼å¼é—®é¢˜
print("\n1. æ•°æ®æ ¼å¼é—®é¢˜æ¼”ç¤º")
print("-" * 30)

# é”™è¯¯çš„æ•°æ®æ ¼å¼
np.random.seed(42)
X_wrong = np.random.uniform(-10, 10, 100).astype(np.float32)  # 1Dæ•°ç»„
y_train = 2 * X_wrong + 1 + np.random.normal(0, 1, 100).astype(np.float32)

print(f"âŒ é”™è¯¯æ ¼å¼ - Xå½¢çŠ¶: {X_wrong.shape}")  # (100,)
print(f"   è¿™æ˜¯1Dæ•°ç»„ï¼Œä½†TensorFlowæœŸæœ›2Dè¾“å…¥")

# æ­£ç¡®çš„æ•°æ®æ ¼å¼
X_correct = X_wrong.reshape(-1, 1)  # è½¬æ¢ä¸º2D
print(f"âœ… æ­£ç¡®æ ¼å¼ - Xå½¢çŠ¶: {X_correct.shape}")  # (100, 1)
print(f"   è¿™æ˜¯2Dæ•°ç»„ï¼Œç¬¦åˆTensorFlowè¦æ±‚")

# 2. æ¼”ç¤ºè®­ç»ƒç¨³å®šæ€§é—®é¢˜
print("\n2. è®­ç»ƒç¨³å®šæ€§å¯¹æ¯”")
print("-" * 30)

def create_and_train_model(X_data, y_data, title, epochs=50):
    """åˆ›å»ºå¹¶è®­ç»ƒæ¨¡å‹"""
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(1, input_shape=(1,))
    ])
    
    model.compile(optimizer='adam', loss='mse', metrics=['mae'])
    
    print(f"\n{title}:")
    try:
        history = model.fit(X_data, y_data, epochs=epochs, verbose=0, batch_size=16)
        
        # è·å–æœ€ç»ˆç»“æœ
        weights, bias = model.layers[0].get_weights()
        final_loss = history.history['loss'][-1]
        
        print(f"  æœ€ç»ˆæŸå¤±: {final_loss:.6f}")
        print(f"  å­¦åˆ°çš„æƒé‡: {weights[0][0]:.4f} (ç›®æ ‡: 2.0)")
        print(f"  å­¦åˆ°çš„åç½®: {bias[0]:.4f} (ç›®æ ‡: 1.0)")
        print(f"  æƒé‡è¯¯å·®: {abs(weights[0][0] - 2.0):.4f}")
        
        return model, history, weights[0][0], bias[0]
        
    except Exception as e:
        print(f"  âŒ è®­ç»ƒå¤±è´¥: {e}")
        return None, None, None, None

# ä½¿ç”¨é”™è¯¯æ ¼å¼è®­ç»ƒï¼ˆå¯èƒ½ä¼šå¤±è´¥ï¼‰
print("å°è¯•ç”¨1Dæ•°æ®è®­ç»ƒ...")
try:
    model_wrong, hist_wrong, w_wrong, b_wrong = create_and_train_model(
        X_wrong, y_train, "âŒ 1Dæ•°æ®æ ¼å¼"
    )
except:
    print("âŒ 1Dæ•°æ®æ ¼å¼: è®­ç»ƒå¤±è´¥ï¼ˆå½¢çŠ¶ä¸åŒ¹é…ï¼‰")
    model_wrong, hist_wrong, w_wrong, b_wrong = None, None, None, None

# ä½¿ç”¨æ­£ç¡®æ ¼å¼è®­ç»ƒ
model_correct, hist_correct, w_correct, b_correct = create_and_train_model(
    X_correct, y_train, "âœ… 2Dæ•°æ®æ ¼å¼"
)

# 3. å¤šæ¬¡è®­ç»ƒå¯¹æ¯”ç¨³å®šæ€§
print("\n3. è®­ç»ƒç¨³å®šæ€§æµ‹è¯•ï¼ˆå¤šæ¬¡è¿è¡Œï¼‰")
print("-" * 30)

weights_list = []
bias_list = []
loss_list = []

for i in range(5):
    # æ¯æ¬¡ä½¿ç”¨ä¸åŒçš„éšæœºç§å­
    tf.random.set_seed(i)
    np.random.seed(i)
    
    model_test = tf.keras.Sequential([
        tf.keras.layers.Dense(1, input_shape=(1,))
    ])
    model_test.compile(optimizer='adam', loss='mse')
    
    history = model_test.fit(X_correct, y_train, epochs=50, verbose=0, batch_size=16)
    weights, bias = model_test.layers[0].get_weights()
    
    weights_list.append(weights[0][0])
    bias_list.append(bias[0])
    loss_list.append(history.history['loss'][-1])
    
    print(f"è¿è¡Œ{i+1}: æƒé‡={weights[0][0]:.4f}, åç½®={bias[0]:.4f}, æŸå¤±={history.history['loss'][-1]:.6f}")

# è®¡ç®—ç¨³å®šæ€§ç»Ÿè®¡
weights_std = np.std(weights_list)
bias_std = np.std(bias_list)
print(f"\nç¨³å®šæ€§åˆ†æ:")
print(f"æƒé‡æ ‡å‡†å·®: {weights_std:.6f} (è¶Šå°è¶Šç¨³å®š)")
print(f"åç½®æ ‡å‡†å·®: {bias_std:.6f} (è¶Šå°è¶Šç¨³å®š)")

# 4. å¯è§†åŒ–ç»“æœ
print("\n4. å¯è§†åŒ–è®­ç»ƒç»“æœ")
print("-" * 30)

if model_correct is not None:
    # ç”Ÿæˆé¢„æµ‹æ•°æ®
    X_test = np.linspace(-10, 10, 100).reshape(-1, 1)
    y_pred = model_correct.predict(X_test, verbose=0)
    
    plt.figure(figsize=(15, 5))
    
    # å­å›¾1ï¼šæ•°æ®å’Œæ‹Ÿåˆç»“æœ
    plt.subplot(1, 3, 1)
    plt.scatter(X_correct.flatten(), y_train, alpha=0.6, label='è®­ç»ƒæ•°æ®')
    plt.plot(X_test.flatten(), y_pred.flatten(), 'r-', linewidth=2, 
             label=f'é¢„æµ‹çº¿ (y={w_correct:.2f}x+{b_correct:.2f})')
    plt.plot(X_test.flatten(), 2*X_test.flatten() + 1, 'g--', linewidth=2, 
             label='çœŸå®çº¿ (y=2x+1)')
    plt.title('æ‹Ÿåˆç»“æœ')
    plt.xlabel('X')
    plt.ylabel('Y')
    plt.legend()
    plt.grid(True)
    
    # å­å›¾2ï¼šè®­ç»ƒè¿‡ç¨‹
    plt.subplot(1, 3, 2)
    plt.plot(hist_correct.history['loss'])
    plt.title('è®­ç»ƒæŸå¤±å˜åŒ–')
    plt.xlabel('è®­ç»ƒè½®æ•°')
    plt.ylabel('æŸå¤±å€¼')
    plt.grid(True)
    
    # å­å›¾3ï¼šå¤šæ¬¡è®­ç»ƒç»“æœåˆ†å¸ƒ
    plt.subplot(1, 3, 3)
    plt.scatter(weights_list, bias_list, c=loss_list, cmap='viridis', s=100)
    plt.axhline(y=1.0, color='r', linestyle='--', alpha=0.7, label='ç›®æ ‡åç½®=1.0')
    plt.axvline(x=2.0, color='r', linestyle='--', alpha=0.7, label='ç›®æ ‡æƒé‡=2.0')
    plt.xlabel('æƒé‡')
    plt.ylabel('åç½®')
    plt.title('å¤šæ¬¡è®­ç»ƒç»“æœåˆ†å¸ƒ')
    plt.colorbar(label='æœ€ç»ˆæŸå¤±')
    plt.legend()
    plt.grid(True)
    
    plt.tight_layout()
    plt.show()

# 5. é—®é¢˜æ€»ç»“å’Œè§£å†³æ–¹æ¡ˆ
print("\n" + "=" * 50)
print("ğŸ“‹ é—®é¢˜æ€»ç»“å’Œè§£å†³æ–¹æ¡ˆ")
print("=" * 50)

print("\nğŸš¨ å¸¸è§é—®é¢˜:")
print("1. æ•°æ®å½¢çŠ¶ä¸åŒ¹é…:")
print("   - é—®é¢˜: Xæ˜¯1Dæ•°ç»„ (100,)ï¼Œä½†Denseå±‚éœ€è¦2Dè¾“å…¥")
print("   - è§£å†³: ä½¿ç”¨ X.reshape(-1, 1) è½¬æ¢ä¸º (100, 1)")

print("\n2. è®­ç»ƒä¸ç¨³å®š:")
print("   - é—®é¢˜: æ¯æ¬¡è®­ç»ƒç»“æœå·®å¼‚å¾ˆå¤§")
print("   - è§£å†³: è®¾ç½®éšæœºç§å­ np.random.seed(42)")

print("\n3. è¯¯å·®å¾ˆå¤§:")
print("   - é—®é¢˜: å­¦åˆ°çš„æƒé‡åç¦»ç›®æ ‡å€¼2.0å¾ˆè¿œ")
print("   - è§£å†³: æ£€æŸ¥æ•°æ®é¢„å¤„ç†ã€è°ƒæ•´å­¦ä¹ ç‡ã€å¢åŠ è®­ç»ƒè½®æ•°")

print("\nâœ… æœ€ä½³å®è·µ:")
print("1. æ€»æ˜¯æ£€æŸ¥æ•°æ®å½¢çŠ¶: print(X.shape)")
print("2. è®¾ç½®éšæœºç§å­ç¡®ä¿å¯é‡ç°æ€§")
print("3. ç›‘æ§è®­ç»ƒè¿‡ç¨‹ä¸­çš„æŸå¤±å€¼å˜åŒ–")
print("4. ä½¿ç”¨éªŒè¯é›†æ£€æŸ¥è¿‡æ‹Ÿåˆ")
print("5. å¤šæ¬¡è¿è¡Œæ£€æŸ¥ç¨³å®šæ€§")

print("\nğŸ”§ ä¿®å¤åçš„ä»£ç æ¨¡æ¿:")
print("""
# æ­£ç¡®çš„çº¿æ€§å›å½’ä»£ç 
np.random.seed(42)  # è®¾ç½®éšæœºç§å­
X_train = np.random.uniform(-10, 10, 100).astype(np.float32)
y_train = 2 * X_train + 1 + np.random.normal(0, 1, 100).astype(np.float32)

# å…³é”®ï¼šé‡å¡‘ä¸º2Dæ ¼å¼
X_train = X_train.reshape(-1, 1)

model = tf.keras.Sequential([
    tf.keras.layers.Dense(1, input_shape=(1,))
])
model.compile(optimizer='adam', loss='mse')

# è®­ç»ƒ
history = model.fit(X_train, y_train, epochs=50, batch_size=16, verbose=1)

# é¢„æµ‹æ—¶ä¹Ÿè¦ä¿æŒ2Dæ ¼å¼
X_test = np.linspace(-10, 10, 50).reshape(-1, 1)
y_pred = model.predict(X_test)
""")

print("\nç°åœ¨è¿è¡Œä¿®å¤åçš„ 01_linear_regression.py åº”è¯¥ä¼šå¾—åˆ°ç¨³å®šçš„ç»“æœï¼")
