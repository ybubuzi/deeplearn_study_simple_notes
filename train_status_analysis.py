"""
ç«è½¦è¿è¡ŒçŠ¶æ€åˆ†æ - æ·±åº¦å­¦ä¹ æ¨¡å‹
åŸºäºæ—¶åºæ•°æ®é¢„æµ‹ç«è½¦è¿è¡ŒçŠ¶æ€

è¿™ä¸ªç¨‹åºçš„ç›®æ ‡ï¼š
1. è¯»å–ç«è½¦è¿è¡Œæ—¥å¿—æ•°æ®ï¼ˆç±»ä¼¼äºwebæœåŠ¡å™¨çš„è®¿é—®æ—¥å¿—ï¼‰
2. æ¸…æ´—å’Œé¢„å¤„ç†æ•°æ®ï¼ˆå°±åƒå¤„ç†JSONæ•°æ®ä¸€æ ·ï¼‰
3. ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹é¢„æµ‹ç«è½¦çš„è¿è¡ŒçŠ¶æ€
4. å¯è§†åŒ–åˆ†æç»“æœ
"""

# å¯¼å…¥å¿…è¦çš„åº“ï¼ˆç±»ä¼¼äºJavaä¸­çš„importæˆ–JSä¸­çš„require/importï¼‰
import pandas as pd           # æ•°æ®å¤„ç†åº“ï¼Œç±»ä¼¼äºExcelæ“ä½œï¼Œç”¨äºè¯»å–å’Œå¤„ç†è¡¨æ ¼æ•°æ®
import numpy as np            # æ•°å­¦è®¡ç®—åº“ï¼Œå¤„ç†æ•°ç»„å’ŒçŸ©é˜µè¿ç®—ï¼ˆç±»ä¼¼äºJavaä¸­çš„æ•°ç»„æ“ä½œï¼‰
import tensorflow as tf       # æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼ŒGoogleå¼€å‘çš„æœºå™¨å­¦ä¹ åº“
from sklearn.preprocessing import LabelEncoder, StandardScaler  # æ•°æ®é¢„å¤„ç†å·¥å…·
from sklearn.model_selection import train_test_split           # æ•°æ®åˆ†å‰²å·¥å…·
import matplotlib.pyplot as plt  # ç»˜å›¾åº“ï¼Œç”¨äºç”Ÿæˆå›¾è¡¨ï¼ˆç±»ä¼¼äºå‰ç«¯çš„å›¾è¡¨åº“ï¼‰
import seaborn as sns            # é«˜çº§ç»˜å›¾åº“ï¼ŒåŸºäºmatplotlib
from datetime import datetime   # æ—¶é—´å¤„ç†åº“
import warnings                  # è­¦å‘Šå¤„ç†åº“
warnings.filterwarnings('ignore')  # å¿½ç•¥è­¦å‘Šä¿¡æ¯ï¼Œè®©è¾“å‡ºæ›´æ¸…æ´

# è®¾ç½®matplotlibç»˜å›¾åº“çš„ä¸­æ–‡å­—ä½“æ”¯æŒï¼ˆå¦åˆ™ä¸­æ–‡ä¼šæ˜¾ç¤ºä¸ºæ–¹å—ï¼‰
plt.rcParams['font.sans-serif'] = ['SimHei']  # è®¾ç½®å­—ä½“ä¸ºé»‘ä½“
plt.rcParams['axes.unicode_minus'] = False    # æ­£å¸¸æ˜¾ç¤ºè´Ÿå·

print("=" * 60)
print("ğŸš‚ ç«è½¦è¿è¡ŒçŠ¶æ€åˆ†æç³»ç»Ÿ")
print("=" * 60)

# å®šä¹‰ç«è½¦çŠ¶æ€åˆ†æå™¨ç±»ï¼ˆç±»ä¼¼äºJavaä¸­çš„classï¼‰
class TrainStatusAnalyzer:
    """
    ç«è½¦çŠ¶æ€åˆ†æå™¨ç±»
    è¿™ä¸ªç±»å°è£…äº†æ‰€æœ‰çš„æ•°æ®å¤„ç†å’Œæœºå™¨å­¦ä¹ åŠŸèƒ½
    ç±»ä¼¼äºJavaä¸­çš„ä¸€ä¸ªServiceç±»ï¼ŒåŒ…å«äº†å®Œæ•´çš„ä¸šåŠ¡é€»è¾‘
    """

    def __init__(self, log_file_path):
        """
        æ„é€ å‡½æ•°ï¼ˆç±»ä¼¼äºJavaçš„æ„é€ å™¨ï¼‰
        åˆå§‹åŒ–åˆ†æå™¨çš„å„ç§å±æ€§

        å‚æ•°:
        log_file_path: æ—¥å¿—æ–‡ä»¶è·¯å¾„ï¼ˆå­—ç¬¦ä¸²ç±»å‹ï¼‰
        """
        # å®ä¾‹å˜é‡ï¼ˆç±»ä¼¼äºJavaä¸­çš„æˆå‘˜å˜é‡ï¼‰
        self.log_file_path = log_file_path    # æ—¥å¿—æ–‡ä»¶è·¯å¾„
        self.df = None                        # åŸå§‹æ•°æ®è¡¨æ ¼ï¼ˆDataFrameï¼Œç±»ä¼¼äºäºŒç»´æ•°ç»„ï¼‰
        self.processed_df = None              # å¤„ç†åçš„æ•°æ®è¡¨æ ¼
        self.label_encoders = {}              # æ ‡ç­¾ç¼–ç å™¨å­—å…¸ï¼ˆç”¨äºå°†æ–‡å­—è½¬æ¢ä¸ºæ•°å­—ï¼‰
        self.scaler = StandardScaler()        # æ•°æ®æ ‡å‡†åŒ–å™¨ï¼ˆå°†æ•°æ®ç¼©æ”¾åˆ°ç›¸åŒèŒƒå›´ï¼‰
        self.model = None                     # æ·±åº¦å­¦ä¹ æ¨¡å‹å¯¹è±¡
        
    def load_and_clean_data(self):
        """
        åŠ è½½å’Œæ¸…æ´—æ•°æ®çš„æ–¹æ³•
        è¿™ä¸ªæ–¹æ³•è´Ÿè´£ä»æ–‡ä»¶ä¸­è¯»å–æ•°æ®å¹¶è¿›è¡Œåˆæ­¥æ¸…æ´—
        ç±»ä¼¼äºä»APIè·å–æ•°æ®å¹¶è¿›è¡Œæ ¼å¼åŒ–å¤„ç†
        """
        print("\nğŸ“Š æ­£åœ¨åŠ è½½å’Œæ¸…æ´—æ•°æ®...")

        # è¯»å–æ—¥å¿—æ–‡ä»¶ï¼ˆç±»ä¼¼äºè¯»å–CSVæ–‡ä»¶æˆ–è§£æJSONï¼‰
        try:
            # ç¬¬ä¸€ç§æ–¹æ³•ï¼šä½¿ç”¨pandasç›´æ¥è¯»å–
            # pandas.read_csv() ç±»ä¼¼äº JavaScript çš„ CSV.parse() æˆ– Java çš„ CSVReader
            self.df = pd.read_csv(
                self.log_file_path,           # æ–‡ä»¶è·¯å¾„
                sep='\s+',                    # åˆ†éš”ç¬¦ï¼šä½¿ç”¨ç©ºç™½å­—ç¬¦ï¼ˆç©ºæ ¼ã€åˆ¶è¡¨ç¬¦ç­‰ï¼‰
                header=0,                     # ç¬¬ä¸€è¡Œä½œä¸ºåˆ—å
                encoding='utf-8',             # æ–‡ä»¶ç¼–ç 
                engine='python'               # ä½¿ç”¨Pythonå¼•æ“è§£æ
            )
        except:
            # å¦‚æœè‡ªåŠ¨è§£æå¤±è´¥ï¼Œæ‰‹åŠ¨è§£ææ–‡ä»¶ï¼ˆç±»ä¼¼äºæ‰‹åŠ¨è§£ææ–‡æœ¬æ–‡ä»¶ï¼‰
            print("è‡ªåŠ¨è§£æå¤±è´¥ï¼Œä½¿ç”¨æ‰‹åŠ¨è§£æ...")

            # æ‰“å¼€æ–‡ä»¶å¹¶è¯»å–æ‰€æœ‰è¡Œï¼ˆç±»ä¼¼äºJavaçš„BufferedReaderæˆ–JSçš„fs.readFileSyncï¼‰
            with open(self.log_file_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()  # è¯»å–æ‰€æœ‰è¡Œåˆ°åˆ—è¡¨ä¸­

            # è§£ææ•°æ®
            data = []  # å­˜å‚¨è§£æåçš„æ•°æ®ï¼ˆäºŒç»´åˆ—è¡¨ï¼‰
            headers = lines[0].strip().split()  # ç¬¬ä¸€è¡Œä½œä¸ºåˆ—å

            # éå†æ¯ä¸€è¡Œæ•°æ®ï¼ˆä»ç¬¬äºŒè¡Œå¼€å§‹ï¼Œè·³è¿‡æ ‡é¢˜è¡Œï¼‰
            for line in lines[1:]:
                # åˆ†å‰²æ¯è¡Œæ•°æ®ï¼Œå»é™¤é¦–å°¾ç©ºç™½å­—ç¬¦
                parts = line.strip().split()

                if len(parts) >= len(headers):
                    # å¦‚æœæ•°æ®åˆ—æ•°è¶…è¿‡æ ‡é¢˜åˆ—æ•°ï¼Œåˆå¹¶æœ€åçš„"å…¶ä»–"åˆ—
                    if len(parts) > len(headers):
                        # å°†å¤šä½™çš„éƒ¨åˆ†åˆå¹¶åˆ°æœ€åä¸€åˆ—ï¼ˆå¤„ç†"å…¶ä»–"å­—æ®µåŒ…å«ç©ºæ ¼çš„æƒ…å†µï¼‰
                        other_data = ' '.join(parts[len(headers)-1:])
                        row = parts[:len(headers)-1] + [other_data]
                    else:
                        row = parts
                    data.append(row)
                else:
                    # å¦‚æœåˆ—æ•°ä¸å¤Ÿï¼Œç”¨ç©ºå­—ç¬¦ä¸²è¡¥é½ç¼ºå¤±çš„åˆ—
                    row = parts + [''] * (len(headers) - len(parts))
                    data.append(row)

            # åˆ›å»ºDataFrameå¯¹è±¡ï¼ˆç±»ä¼¼äºåˆ›å»ºä¸€ä¸ªäºŒç»´è¡¨æ ¼ï¼‰
            # DataFrame å¯ä»¥ç†è§£ä¸ºç±»ä¼¼äº JavaScript çš„å¯¹è±¡æ•°ç»„æˆ– Java çš„ List<Map<String, Object>>
            self.df = pd.DataFrame(data, columns=headers)

        # æ‰“å°æ•°æ®åŸºæœ¬ä¿¡æ¯
        print(f"åŸå§‹æ•°æ®å½¢çŠ¶: {self.df.shape}")  # shape è¿”å› (è¡Œæ•°, åˆ—æ•°) çš„å…ƒç»„
        print(f"åˆ—å: {list(self.df.columns)}")   # æ˜¾ç¤ºæ‰€æœ‰åˆ—å

        # è°ƒç”¨æ•°æ®æ¸…æ´—æ–¹æ³•
        self._clean_data()
        
    def _clean_data(self):
        """
        æ¸…æ´—æ•°æ®çš„ç§æœ‰æ–¹æ³•ï¼ˆæ–¹æ³•åå‰çš„ä¸‹åˆ’çº¿è¡¨ç¤ºç§æœ‰ï¼Œç±»ä¼¼äºJavaçš„privateï¼‰
        è¿™ä¸ªæ–¹æ³•è´Ÿè´£å¤„ç†è„æ•°æ®ã€ç¼ºå¤±å€¼ã€æ•°æ®ç±»å‹è½¬æ¢ç­‰
        """
        print("\nğŸ§¹ æ­£åœ¨æ¸…æ´—æ•°æ®...")

        # å¤„ç†æ•°å€¼åˆ—ï¼ˆå°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å­—ï¼‰
        # å®šä¹‰å“ªäº›åˆ—åº”è¯¥æ˜¯æ•°å€¼ç±»å‹
        numeric_columns = ['åºå·', 'è·ç¦»', 'é€Ÿåº¦', 'é™é€Ÿ', 'ç®¡å‹']

        for col in numeric_columns:
            if col in self.df.columns:  # æ£€æŸ¥åˆ—æ˜¯å¦å­˜åœ¨
                # pd.to_numeric() ç±»ä¼¼äº JavaScript çš„ parseInt() æˆ– Java çš„ Integer.parseInt()
                # errors='coerce' è¡¨ç¤ºæ— æ³•è½¬æ¢çš„å€¼è®¾ä¸º NaNï¼ˆNot a Numberï¼‰
                # fillna(0) å°† NaN å€¼å¡«å……ä¸º 0
                self.df[col] = pd.to_numeric(self.df[col], errors='coerce').fillna(0)

        # å¤„ç†æ—¶é—´åˆ—ï¼ˆæ—¶é—´å­—ç¬¦ä¸²è½¬æ¢ä¸ºæ—¶é—´å¯¹è±¡ï¼‰
        if 'æ—¥æœŸæ—¶é—´' in self.df.columns:
            try:
                # pd.to_datetime() ç±»ä¼¼äº JavaScript çš„ new Date() æˆ– Java çš„ SimpleDateFormat.parse()
                # format å‚æ•°æŒ‡å®šæ—¶é—´æ ¼å¼ï¼š%Y=å¹´ï¼Œ%m=æœˆï¼Œ%d=æ—¥ï¼Œ%H=æ—¶ï¼Œ%M=åˆ†ï¼Œ%S=ç§’
                self.df['æ—¥æœŸæ—¶é—´'] = pd.to_datetime(self.df['æ—¥æœŸæ—¶é—´'], format='%Y-%m-%d%H:%M:%S')
            except:
                print("âš ï¸ æ—¶é—´æ ¼å¼è§£æå¤±è´¥ï¼Œä½¿ç”¨åºå·ä½œä¸ºæ—¶é—´åºåˆ—")
                # å¦‚æœæ—¶é—´è§£æå¤±è´¥ï¼Œåˆ›å»ºä¸€ä¸ªé€’å¢çš„åºåˆ—ä½œä¸ºæ—¶é—´è½´
                # range(len(self.df)) ç”Ÿæˆ 0, 1, 2, 3, ... çš„åºåˆ—
                self.df['æ—¶é—´åºåˆ—'] = range(len(self.df))

        # å¤„ç†ç¼ºå¤±å€¼ï¼ˆå°†æ‰€æœ‰ NaN æˆ– None å€¼å¡«å……ä¸ºç©ºå­—ç¬¦ä¸²ï¼‰
        # fillna('') ç±»ä¼¼äº JavaScript çš„ || '' æˆ– Java çš„ Optional.orElse("")
        self.df = self.df.fillna('')

        # åˆ›å»ºçŠ¶æ€æ ‡ç­¾ï¼ˆè¿™æ˜¯æœºå™¨å­¦ä¹ çš„ç›®æ ‡å˜é‡ï¼‰
        self._create_status_labels()

        print(f"æ¸…æ´—åæ•°æ®å½¢çŠ¶: {self.df.shape}")
        
    def _create_status_labels(self):
        """
        æ ¹æ®äº‹ä»¶ç±»å‹å’Œå‚æ•°åˆ›å»ºçŠ¶æ€æ ‡ç­¾
        è¿™æ˜¯æœºå™¨å­¦ä¹ ä¸­çš„"æ ‡ç­¾å·¥ç¨‹"ï¼Œå°†åŸå§‹æ•°æ®è½¬æ¢ä¸ºå¯ä»¥é¢„æµ‹çš„ç›®æ ‡ç±»åˆ«
        ç±»ä¼¼äºç»™æ¯æ¡è®°å½•æ‰“ä¸Šä¸€ä¸ª"æ ‡ç­¾"ï¼Œå‘Šè¯‰æ¨¡å‹è¿™æ¡è®°å½•ä»£è¡¨ä»€ä¹ˆçŠ¶æ€
        """
        print("\nğŸ·ï¸ æ­£åœ¨åˆ›å»ºçŠ¶æ€æ ‡ç­¾...")

        def classify_status(row):
            """
            çŠ¶æ€åˆ†ç±»å‡½æ•°ï¼ˆå†…éƒ¨å‡½æ•°ï¼Œç±»ä¼¼äºJavaScriptçš„å†…éƒ¨å‡½æ•°æˆ–Javaçš„å†…éƒ¨æ–¹æ³•ï¼‰
            æ ¹æ®è®°å½•åç§°ã€é€Ÿåº¦ã€ä¿¡å·ç¯ç­‰ä¿¡æ¯åˆ†ç±»çŠ¶æ€

            å‚æ•°:
            row: DataFrameçš„ä¸€è¡Œæ•°æ®ï¼ˆç±»ä¼¼äºä¸€ä¸ªå¯¹è±¡ï¼ŒåŒ…å«æ‰€æœ‰åˆ—çš„å€¼ï¼‰

            è¿”å›:
            å­—ç¬¦ä¸²ç±»å‹çš„çŠ¶æ€æ ‡ç­¾
            """
            # æå–å…³é”®å­—æ®µï¼ˆç±»ä¼¼äºä»å¯¹è±¡ä¸­å–å€¼ï¼‰
            record_name = str(row['è®°å½•åç§°']).lower()  # è½¬æ¢ä¸ºå°å†™ä¾¿äºåŒ¹é…
            speed = float(row['é€Ÿåº¦']) if row['é€Ÿåº¦'] != '' else 0  # å®‰å…¨è½¬æ¢ä¸ºæµ®ç‚¹æ•°
            signal = str(row['ä¿¡å·ç¯'])

            # å®šä¹‰çŠ¶æ€åˆ†ç±»è§„åˆ™ï¼ˆç±»ä¼¼äºä¸€ç³»åˆ—if-elseåˆ¤æ–­ï¼‰
            # è¿™äº›è§„åˆ™æ˜¯åŸºäºä¸šåŠ¡é€»è¾‘åˆ¶å®šçš„ï¼Œç±»ä¼¼äºä¸šåŠ¡è§„åˆ™å¼•æ“
            if 'å¼€æœº' in record_name or 'è¿›å…¥' in record_name:
                return 'åˆå§‹åŒ–'
            elif 'è‡ªæ£€' in record_name or 'åˆ¶åŠ¨' in record_name:
                return 'æ£€æµ‹çŠ¶æ€'
            elif speed == 0 and ('åœè½¦' in record_name or 'åœ' in record_name):
                return 'åœè½¦çŠ¶æ€'
            elif speed > 0 and speed <= 30:
                return 'ä½é€Ÿè¿è¡Œ'
            elif speed > 30 and speed <= 60:
                return 'ä¸­é€Ÿè¿è¡Œ'
            elif speed > 60:
                return 'é«˜é€Ÿè¿è¡Œ'
            elif 'çº¢' in signal:
                return 'åœè½¦ä¿¡å·'
            elif 'é»„' in signal:
                return 'å‡é€Ÿä¿¡å·'
            elif 'ç»¿' in signal:
                return 'æ­£å¸¸è¿è¡Œ'
            elif 'æŒ‰é”®æ“ä½œ' in record_name:
                return 'æ“ä½œçŠ¶æ€'
            elif 'ç®¡å‹å˜åŒ–' in record_name or 'é€Ÿåº¦å˜åŒ–' in record_name:
                return 'å‚æ•°å˜åŒ–'
            else:
                return 'å…¶ä»–çŠ¶æ€'

        # åº”ç”¨åˆ†ç±»å‡½æ•°åˆ°æ¯ä¸€è¡Œæ•°æ®
        # df.apply() ç±»ä¼¼äº JavaScript çš„ array.map() æˆ– Java çš„ stream().map()
        # axis=1 è¡¨ç¤ºæŒ‰è¡Œåº”ç”¨å‡½æ•°ï¼ˆaxis=0æ˜¯æŒ‰åˆ—ï¼‰
        self.df['è¿è¡ŒçŠ¶æ€'] = self.df.apply(classify_status, axis=1)

        # ç»Ÿè®¡å„çŠ¶æ€çš„åˆ†å¸ƒæƒ…å†µ
        # value_counts() ç±»ä¼¼äº JavaScript çš„ reduce() è®¡æ•°æˆ– Java çš„ groupingBy() + counting()
        status_counts = self.df['è¿è¡ŒçŠ¶æ€'].value_counts()
        print("çŠ¶æ€åˆ†å¸ƒ:")
        for status, count in status_counts.items():
            print(f"  {status}: {count}æ¡è®°å½•")
            
    def feature_engineering(self):
        """
        ç‰¹å¾å·¥ç¨‹æ–¹æ³•
        è¿™æ˜¯æœºå™¨å­¦ä¹ ä¸­æœ€é‡è¦çš„æ­¥éª¤ä¹‹ä¸€ï¼Œå°†åŸå§‹æ•°æ®è½¬æ¢ä¸ºæ¨¡å‹å¯ä»¥ç†è§£çš„ç‰¹å¾
        ç±»ä¼¼äºå‰ç«¯å¼€å‘ä¸­çš„æ•°æ®é¢„å¤„ç†ï¼Œæˆ–è€…åç«¯å¼€å‘ä¸­çš„DTOè½¬æ¢
        """
        print("\nâš™ï¸ æ­£åœ¨è¿›è¡Œç‰¹å¾å·¥ç¨‹...")

        # é€‰æ‹©åŸºç¡€æ•°å€¼ç‰¹å¾åˆ—ï¼ˆè¿™äº›æ˜¯å¯ä»¥ç›´æ¥ç”¨äºè®­ç»ƒçš„æ•°å€¼å‹æ•°æ®ï¼‰
        feature_columns = ['é€Ÿåº¦', 'é™é€Ÿ', 'ç®¡å‹', 'è·ç¦»']

        # å®šä¹‰éœ€è¦ç¼–ç çš„åˆ†ç±»ç‰¹å¾ï¼ˆæ–‡æœ¬ç±»å‹çš„æ•°æ®éœ€è¦è½¬æ¢ä¸ºæ•°å­—ï¼‰
        # æœºå™¨å­¦ä¹ æ¨¡å‹åªèƒ½å¤„ç†æ•°å­—ï¼Œä¸èƒ½ç›´æ¥å¤„ç†æ–‡å­—
        categorical_features = ['è®°å½•åç§°', 'ä¿¡å·ç¯', 'ç©ºæŒ¡', 'å‰å', 'è½¦ç«™å']

        # å¤åˆ¶åŸå§‹æ•°æ®ï¼Œé¿å…ä¿®æ”¹åŸå§‹æ•°æ®ï¼ˆç±»ä¼¼äºæ·±æ‹·è´ï¼‰
        processed_data = self.df.copy()

        # å¯¹åˆ†ç±»ç‰¹å¾è¿›è¡Œæ ‡ç­¾ç¼–ç ï¼ˆå°†æ–‡å­—è½¬æ¢ä¸ºæ•°å­—ï¼‰
        for col in categorical_features:
            if col in processed_data.columns:
                # LabelEncoder ç±»ä¼¼äºåˆ›å»ºä¸€ä¸ªå­—å…¸æ˜ å°„
                # ä¾‹å¦‚ï¼š{'çº¢ç¯': 0, 'ç»¿ç¯': 1, 'é»„ç¯': 2}
                le = LabelEncoder()

                # ç¡®ä¿æ‰€æœ‰å€¼éƒ½æ˜¯å­—ç¬¦ä¸²ç±»å‹
                processed_data[col] = processed_data[col].astype(str)

                # è¿›è¡Œç¼–ç è½¬æ¢
                # fit_transform() ç›¸å½“äºå…ˆå­¦ä¹ æ˜ å°„å…³ç³»ï¼Œå†åº”ç”¨è½¬æ¢
                processed_data[f'{col}_encoded'] = le.fit_transform(processed_data[col])

                # ä¿å­˜ç¼–ç å™¨ï¼Œä»¥ä¾¿åç»­é¢„æµ‹æ—¶ä½¿ç”¨ç›¸åŒçš„æ˜ å°„
                self.label_encoders[col] = le

                # å°†ç¼–ç åçš„åˆ—æ·»åŠ åˆ°ç‰¹å¾åˆ—è¡¨
                feature_columns.append(f'{col}_encoded')

        # åˆ›å»ºæ—¶é—´ç‰¹å¾ï¼ˆä»æ—¶é—´æˆ³ä¸­æå–æœ‰ç”¨çš„æ—¶é—´ä¿¡æ¯ï¼‰
        if 'æ—¥æœŸæ—¶é—´' in processed_data.columns:
            # æå–å°æ—¶å’Œåˆ†é’Ÿä½œä¸ºç‰¹å¾ï¼ˆç±»ä¼¼äºä»Dateå¯¹è±¡ä¸­æå–æ—¶é—´éƒ¨åˆ†ï¼‰
            processed_data['å°æ—¶'] = processed_data['æ—¥æœŸæ—¶é—´'].dt.hour
            processed_data['åˆ†é’Ÿ'] = processed_data['æ—¥æœŸæ—¶é—´'].dt.minute
            feature_columns.extend(['å°æ—¶', 'åˆ†é’Ÿ'])
        else:
            # å¦‚æœæ²¡æœ‰æ—¶é—´åˆ—ï¼Œä½¿ç”¨åºå·åˆ›å»ºæ—¶é—´ç‰¹å¾
            processed_data['æ—¶é—´åºåˆ—'] = range(len(processed_data))
            feature_columns.append('æ—¶é—´åºåˆ—')

        # åˆ›å»ºæ»‘åŠ¨çª—å£ç‰¹å¾ï¼ˆæ—¶åºæ•°æ®çš„é‡è¦ç‰¹å¾ï¼‰
        # æ»‘åŠ¨çª—å£å¯ä»¥æ•æ‰æ•°æ®çš„è¶‹åŠ¿å’Œæ¨¡å¼
        window_size = 5  # çª—å£å¤§å°ï¼šä½¿ç”¨å‰5ä¸ªæ•°æ®ç‚¹

        for col in ['é€Ÿåº¦', 'ç®¡å‹']:
            if col in processed_data.columns:
                # æ»‘åŠ¨å¹³å‡ï¼šè®¡ç®—è¿‡å»5ä¸ªæ—¶é—´ç‚¹çš„å¹³å‡å€¼
                # rolling() ç±»ä¼¼äºåˆ›å»ºä¸€ä¸ªæ»‘åŠ¨çª—å£
                # ä¾‹å¦‚ï¼š[1,2,3,4,5] çš„æ»‘åŠ¨å¹³å‡(çª—å£=3) = [1, 1.5, 2, 3, 4]
                processed_data[f'{col}_æ»‘åŠ¨å¹³å‡'] = processed_data[col].rolling(
                    window=window_size, min_periods=1
                ).mean()

                # å˜åŒ–ç‡ï¼šè®¡ç®—ç›¸å¯¹äºå‰ä¸€ä¸ªæ—¶é—´ç‚¹çš„å˜åŒ–ç™¾åˆ†æ¯”
                # pct_change() ç±»ä¼¼äº (current - previous) / previous
                processed_data[f'{col}_å˜åŒ–ç‡'] = processed_data[col].pct_change().fillna(0)

                # æ·»åŠ åˆ°ç‰¹å¾åˆ—è¡¨
                feature_columns.extend([f'{col}_æ»‘åŠ¨å¹³å‡', f'{col}_å˜åŒ–ç‡'])

        # é€‰æ‹©æœ€ç»ˆçš„ç‰¹å¾åˆ—ï¼ˆåªä¿ç•™å­˜åœ¨çš„åˆ—ï¼‰
        available_features = [col for col in feature_columns if col in processed_data.columns]

        # åˆ›å»ºæœ€ç»ˆçš„å¤„ç†åæ•°æ®é›†ï¼ˆåŒ…å«ç‰¹å¾å’Œç›®æ ‡å˜é‡ï¼‰
        self.processed_df = processed_data[available_features + ['è¿è¡ŒçŠ¶æ€']].copy()

        # å¤„ç†å¼‚å¸¸å€¼ï¼šæ— ç©·å¤§å’ŒNaNå€¼
        # replace() å°†æ— ç©·å¤§å€¼æ›¿æ¢ä¸ºNaNï¼Œç„¶åfillna(0)å°†NaNæ›¿æ¢ä¸º0
        self.processed_df = self.processed_df.replace([np.inf, -np.inf], np.nan).fillna(0)

        print(f"ç‰¹å¾å·¥ç¨‹å®Œæˆï¼Œæœ€ç»ˆç‰¹å¾æ•°: {len(available_features)}")
        print(f"ç‰¹å¾åˆ—è¡¨: {available_features}")

        """
        ç‰¹å¾å·¥ç¨‹åçš„æ•°æ®ç»“æ„ç¤ºä¾‹ï¼š

        åŸå§‹æ•°æ®ï¼š
        | è®°å½•åç§° | é€Ÿåº¦ | ä¿¡å·ç¯ | è¿è¡ŒçŠ¶æ€ |
        |---------|------|--------|----------|
        | å¼€æœº    | 0    | çº¢ç¯   | åˆå§‹åŒ–   |
        | åŠ é€Ÿ    | 30   | ç»¿ç¯   | ä½é€Ÿè¿è¡Œ |

        å¤„ç†åæ•°æ®ï¼š
        | é€Ÿåº¦ | è®°å½•åç§°_encoded | ä¿¡å·ç¯_encoded | é€Ÿåº¦_æ»‘åŠ¨å¹³å‡ | é€Ÿåº¦_å˜åŒ–ç‡ | è¿è¡ŒçŠ¶æ€ |
        |------|------------------|----------------|---------------|-------------|----------|
        | 0    | 0                | 0              | 0             | 0           | åˆå§‹åŒ–   |
        | 30   | 1                | 1              | 15            | inf         | ä½é€Ÿè¿è¡Œ |
        """
        
    def prepare_sequences(self, sequence_length=10):
        """
        å‡†å¤‡æ—¶åºæ•°æ®çš„æ–¹æ³•
        å°†æ™®é€šçš„è¡¨æ ¼æ•°æ®è½¬æ¢ä¸ºæ—¶åºåºåˆ—æ•°æ®ï¼Œä¾›LSTMç­‰æ—¶åºæ¨¡å‹ä½¿ç”¨

        æ—¶åºæ•°æ®çš„æ¦‚å¿µï¼š
        - æ™®é€šæ•°æ®ï¼šæ¯ä¸€è¡Œæ˜¯ç‹¬ç«‹çš„æ ·æœ¬
        - æ—¶åºæ•°æ®ï¼šæ¯ä¸ªæ ·æœ¬åŒ…å«è¿ç»­çš„å¤šä¸ªæ—¶é—´ç‚¹çš„æ•°æ®

        ä¾‹å¦‚ï¼šé¢„æµ‹ç¬¬11ä¸ªæ—¶é—´ç‚¹çš„çŠ¶æ€ï¼Œéœ€è¦çœ‹å‰10ä¸ªæ—¶é—´ç‚¹çš„æ•°æ®
        ç±»ä¼¼äºæ ¹æ®è¿‡å»10å¤©çš„è‚¡ä»·é¢„æµ‹æ˜å¤©çš„è‚¡ä»·
        """
        print(f"\nğŸ“ˆ æ­£åœ¨å‡†å¤‡æ—¶åºæ•°æ®ï¼Œåºåˆ—é•¿åº¦: {sequence_length}")

        # åˆ†ç¦»ç‰¹å¾å’Œæ ‡ç­¾ï¼ˆç±»ä¼¼äºåˆ†ç¦»è¾“å…¥å’Œè¾“å‡ºï¼‰
        # ç‰¹å¾ï¼šç”¨äºé¢„æµ‹çš„è¾“å…¥æ•°æ®ï¼ˆXï¼‰
        # æ ‡ç­¾ï¼šè¦é¢„æµ‹çš„ç›®æ ‡æ•°æ®ï¼ˆyï¼‰
        feature_cols = [col for col in self.processed_df.columns if col != 'è¿è¡ŒçŠ¶æ€']
        X = self.processed_df[feature_cols].values  # .values å°†DataFrameè½¬æ¢ä¸ºnumpyæ•°ç»„

        # ç¼–ç ç›®æ ‡å˜é‡ï¼ˆå°†çŠ¶æ€æ–‡å­—è½¬æ¢ä¸ºæ•°å­—ï¼‰
        # ä¾‹å¦‚ï¼š['åˆå§‹åŒ–', 'ä½é€Ÿè¿è¡Œ', 'é«˜é€Ÿè¿è¡Œ'] -> [0, 1, 2]
        le_target = LabelEncoder()
        y = le_target.fit_transform(self.processed_df['è¿è¡ŒçŠ¶æ€'])
        self.label_encoders['target'] = le_target  # ä¿å­˜ç¼–ç å™¨

        # æ ‡å‡†åŒ–ç‰¹å¾ï¼ˆå°†æ‰€æœ‰ç‰¹å¾ç¼©æ”¾åˆ°ç›¸åŒèŒƒå›´ï¼Œé€šå¸¸æ˜¯0-1æˆ–-1åˆ°1ï¼‰
        # è¿™å¾ˆé‡è¦ï¼Œå› ä¸ºä¸åŒç‰¹å¾çš„æ•°å€¼èŒƒå›´å¯èƒ½å·®å¼‚å¾ˆå¤§
        # ä¾‹å¦‚ï¼šé€Ÿåº¦(0-100) vs ç®¡å‹(0-500)ï¼Œéœ€è¦æ ‡å‡†åŒ–åˆ°ç›¸åŒèŒƒå›´
        X_scaled = self.scaler.fit_transform(X)

        # åˆ›å»ºæ—¶åºåºåˆ—
        # è¿™æ˜¯æ—¶åºå»ºæ¨¡çš„æ ¸å¿ƒï¼šå°†è¿ç»­çš„æ•°æ®ç‚¹ç»„åˆæˆåºåˆ—
        X_sequences = []  # å­˜å‚¨è¾“å…¥åºåˆ—
        y_sequences = []  # å­˜å‚¨å¯¹åº”çš„æ ‡ç­¾

        # ä»ç¬¬sequence_lengthä¸ªæ•°æ®ç‚¹å¼€å§‹ï¼Œå› ä¸ºå‰é¢çš„ç‚¹ç”¨ä½œå†å²æ•°æ®
        for i in range(sequence_length, len(X_scaled)):
            # å–å‰sequence_lengthä¸ªæ—¶é—´ç‚¹ä½œä¸ºè¾“å…¥åºåˆ—
            # ä¾‹å¦‚ï¼ši=10æ—¶ï¼Œå–ç´¢å¼•0-9çš„æ•°æ®ä½œä¸ºè¾“å…¥ï¼Œé¢„æµ‹ç´¢å¼•10çš„æ ‡ç­¾
            X_sequences.append(X_scaled[i-sequence_length:i])
            y_sequences.append(y[i])

        # è½¬æ¢ä¸ºnumpyæ•°ç»„ï¼ˆæ·±åº¦å­¦ä¹ æ¡†æ¶éœ€è¦çš„æ ¼å¼ï¼‰
        X_sequences = np.array(X_sequences)
        y_sequences = np.array(y_sequences)

        print(f"åºåˆ—æ•°æ®å½¢çŠ¶: X={X_sequences.shape}, y={y_sequences.shape}")

        """
        æ•°æ®å½¢çŠ¶è¯´æ˜ï¼š
        X_sequences.shape = (æ ·æœ¬æ•°, åºåˆ—é•¿åº¦, ç‰¹å¾æ•°)
        ä¾‹å¦‚ï¼š(1000, 10, 15) è¡¨ç¤ºï¼š
        - 1000ä¸ªæ ·æœ¬
        - æ¯ä¸ªæ ·æœ¬åŒ…å«10ä¸ªæ—¶é—´æ­¥
        - æ¯ä¸ªæ—¶é—´æ­¥æœ‰15ä¸ªç‰¹å¾

        è¿™ç±»ä¼¼äºä¸€ä¸ªä¸‰ç»´æ•°ç»„ï¼š
        [
          [ [ç‰¹å¾1, ç‰¹å¾2, ...], [ç‰¹å¾1, ç‰¹å¾2, ...], ... ],  # æ ·æœ¬1çš„10ä¸ªæ—¶é—´æ­¥
          [ [ç‰¹å¾1, ç‰¹å¾2, ...], [ç‰¹å¾1, ç‰¹å¾2, ...], ... ],  # æ ·æœ¬2çš„10ä¸ªæ—¶é—´æ­¥
          ...
        ]
        """

        return X_sequences, y_sequences
        
    def build_model(self, input_shape, num_classes):
        """
        æ„å»ºLSTMæ·±åº¦å­¦ä¹ æ¨¡å‹
        LSTM (Long Short-Term Memory) æ˜¯ä¸€ç§ç‰¹æ®Šçš„ç¥ç»ç½‘ç»œï¼Œæ“…é•¿å¤„ç†æ—¶åºæ•°æ®
        ç±»ä¼¼äºäººçš„è®°å¿†ï¼šèƒ½è®°ä½é‡è¦çš„å†å²ä¿¡æ¯ï¼Œå¿˜è®°ä¸é‡è¦çš„ä¿¡æ¯
        """
        print(f"\nğŸ§  æ­£åœ¨æ„å»ºLSTMæ¨¡å‹...")
        print(f"è¾“å…¥å½¢çŠ¶: {input_shape}, ç±»åˆ«æ•°: {num_classes}")

        # ä½¿ç”¨Sequentialæ¨¡å‹ï¼ˆå±‚çº§å¼å †å ï¼Œç±»ä¼¼äºæ­ç§¯æœ¨ï¼‰
        model = tf.keras.Sequential([

            # ç¬¬ä¸€ä¸ªLSTMå±‚
            # LSTMå±‚æ˜¯æ¨¡å‹çš„"è®°å¿†å•å…ƒ"ï¼Œèƒ½å¤Ÿå­¦ä¹ æ—¶åºæ¨¡å¼
            # 64: ç¥ç»å…ƒæ•°é‡ï¼ˆç±»ä¼¼äºå¤§è„‘ä¸­çš„ç¥ç»å…ƒæ•°é‡ï¼‰
            # return_sequences=True: è¾“å‡ºå®Œæ•´åºåˆ—ï¼ˆä¸ºä¸‹ä¸€å±‚LSTMæä¾›è¾“å…¥ï¼‰
            tf.keras.layers.LSTM(64, return_sequences=True, input_shape=input_shape, name='lstm1'),

            # Dropoutå±‚ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆï¼ˆç±»ä¼¼äºéšæœº"å…³é—­"ä¸€äº›ç¥ç»å…ƒï¼‰
            # 0.2è¡¨ç¤ºéšæœºå…³é—­20%çš„ç¥ç»å…ƒï¼Œç±»ä¼¼äºæ­£åˆ™åŒ–
            tf.keras.layers.Dropout(0.2, name='dropout1'),

            # ç¬¬äºŒä¸ªLSTMå±‚
            # 32: æ›´å°‘çš„ç¥ç»å…ƒï¼Œé€æ¸å‹ç¼©ä¿¡æ¯
            # return_sequences=False: åªè¾“å‡ºæœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„ç»“æœ
            tf.keras.layers.LSTM(32, return_sequences=False, name='lstm2'),
            tf.keras.layers.Dropout(0.2, name='dropout2'),

            # å…¨è¿æ¥å±‚ï¼ˆDenseå±‚ï¼‰
            # å°†LSTMçš„è¾“å‡ºè¿›ä¸€æ­¥å¤„ç†ï¼Œç±»ä¼¼äºä¼ ç»Ÿçš„ç¥ç»ç½‘ç»œå±‚
            # 32: ç¥ç»å…ƒæ•°é‡
            # activation='relu': æ¿€æ´»å‡½æ•°ï¼Œç±»ä¼¼äºå¼€å…³ï¼Œå†³å®šç¥ç»å…ƒæ˜¯å¦æ¿€æ´»
            tf.keras.layers.Dense(32, activation='relu', name='dense1'),
            tf.keras.layers.Dropout(0.3, name='dropout3'),

            # è¾“å‡ºå±‚
            # num_classes: è¾“å‡ºç±»åˆ«æ•°ï¼ˆæœ‰å¤šå°‘ç§çŠ¶æ€å°±æœ‰å¤šå°‘ä¸ªç¥ç»å…ƒï¼‰
            # activation='softmax': å°†è¾“å‡ºè½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒï¼ˆæ‰€æœ‰æ¦‚ç‡åŠ èµ·æ¥=1ï¼‰
            tf.keras.layers.Dense(num_classes, activation='softmax', name='output')
        ])

        # ç¼–è¯‘æ¨¡å‹ï¼ˆè®¾ç½®è®­ç»ƒå‚æ•°ï¼‰
        model.compile(
            # ä¼˜åŒ–å™¨ï¼šæ§åˆ¶æ¨¡å‹å¦‚ä½•å­¦ä¹ ï¼ˆç±»ä¼¼äºå­¦ä¹ ç­–ç•¥ï¼‰
            optimizer='adam',  # Adamæ˜¯ä¸€ç§è‡ªé€‚åº”å­¦ä¹ ç‡çš„ä¼˜åŒ–å™¨

            # æŸå¤±å‡½æ•°ï¼šè¡¡é‡é¢„æµ‹ä¸çœŸå®å€¼çš„å·®è·ï¼ˆç±»ä¼¼äºè€ƒè¯•è¯„åˆ†æ ‡å‡†ï¼‰
            loss='sparse_categorical_crossentropy',  # é€‚ç”¨äºå¤šåˆ†ç±»é—®é¢˜

            # è¯„ä¼°æŒ‡æ ‡ï¼šè®­ç»ƒè¿‡ç¨‹ä¸­ç›‘æ§çš„æŒ‡æ ‡
            metrics=['accuracy']  # å‡†ç¡®ç‡ï¼šé¢„æµ‹æ­£ç¡®çš„æ¯”ä¾‹
        )

        print("æ¨¡å‹ç»“æ„:")
        model.summary()  # æ˜¾ç¤ºæ¨¡å‹çš„è¯¦ç»†ç»“æ„

        """
        æ¨¡å‹ç»“æ„è§£é‡Šï¼š

        è¾“å…¥: (batch_size, 10, 15) - æ‰¹æ¬¡å¤§å° x æ—¶é—´æ­¥æ•° x ç‰¹å¾æ•°
        â†“
        LSTM1(64): å­¦ä¹ æ—¶åºæ¨¡å¼ï¼Œè¾“å‡º (batch_size, 10, 64)
        â†“
        Dropout(0.2): éšæœºå…³é—­20%ç¥ç»å…ƒ
        â†“
        LSTM2(32): è¿›ä¸€æ­¥å­¦ä¹ ï¼Œè¾“å‡º (batch_size, 32)
        â†“
        Dropout(0.2): é˜²æ­¢è¿‡æ‹Ÿåˆ
        â†“
        Dense(32): å…¨è¿æ¥å±‚ï¼Œè¾“å‡º (batch_size, 32)
        â†“
        Dropout(0.3): æ›´å¼ºçš„æ­£åˆ™åŒ–
        â†“
        Dense(num_classes): è¾“å‡ºå±‚ï¼Œè¾“å‡º (batch_size, num_classes)
        â†“
        Softmax: è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒ
        """

        return model

    def train_model(self, X, y, test_size=0.2, epochs=50):
        """
        è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹
        è¿™æ˜¯æœºå™¨å­¦ä¹ çš„æ ¸å¿ƒæ­¥éª¤ï¼šè®©æ¨¡å‹ä»æ•°æ®ä¸­å­¦ä¹ è§„å¾‹
        ç±»ä¼¼äºè®©å­¦ç”Ÿé€šè¿‡åšç»ƒä¹ é¢˜æ¥å­¦ä¹ çŸ¥è¯†

        å‚æ•°:
        X: è¾“å…¥ç‰¹å¾æ•°æ®ï¼ˆä¸‰ç»´æ•°ç»„ï¼‰
        y: ç›®æ ‡æ ‡ç­¾æ•°æ®ï¼ˆä¸€ç»´æ•°ç»„ï¼‰
        test_size: æµ‹è¯•é›†æ¯”ä¾‹ï¼ˆ0.2è¡¨ç¤º20%ç”¨äºæµ‹è¯•ï¼‰
        epochs: è®­ç»ƒè½®æ•°ï¼ˆæ¨¡å‹çœ‹æ•°æ®çš„æ¬¡æ•°ï¼‰
        """
        print(f"\nğŸš€ å¼€å§‹è®­ç»ƒæ¨¡å‹...")

        # åˆ†å‰²æ•°æ®é›†ï¼ˆç±»ä¼¼äºå°†é¢˜ç›®åˆ†ä¸ºç»ƒä¹ é¢˜å’Œè€ƒè¯•é¢˜ï¼‰
        # train_test_split ç±»ä¼¼äºéšæœºåˆ†é…ï¼Œç¡®ä¿è®­ç»ƒå’Œæµ‹è¯•æ•°æ®çš„ä»£è¡¨æ€§
        X_train, X_test, y_train, y_test = train_test_split(
            X, y,                    # è¦åˆ†å‰²çš„æ•°æ®
            test_size=test_size,     # æµ‹è¯•é›†æ¯”ä¾‹
            random_state=42,         # éšæœºç§å­ï¼Œç¡®ä¿ç»“æœå¯é‡ç°
            stratify=y               # åˆ†å±‚æŠ½æ ·ï¼Œç¡®ä¿å„ç±»åˆ«æ¯”ä¾‹ä¸€è‡´
        )

        print(f"è®­ç»ƒé›†å¤§å°: {X_train.shape[0]}")
        print(f"æµ‹è¯•é›†å¤§å°: {X_test.shape[0]}")

        # æ„å»ºæ¨¡å‹
        num_classes = len(np.unique(y))  # è®¡ç®—ç±»åˆ«æ•°é‡
        self.model = self.build_model(X_train.shape[1:], num_classes)

        # è®­ç»ƒæ¨¡å‹ï¼ˆè¿™æ˜¯æœ€é‡è¦çš„æ­¥éª¤ï¼‰
        print("å¼€å§‹è®­ç»ƒè¿‡ç¨‹...")
        history = self.model.fit(
            X_train, y_train,           # è®­ç»ƒæ•°æ®
            epochs=epochs,              # è®­ç»ƒè½®æ•°ï¼ˆæ¨¡å‹çœ‹æ•°æ®çš„æ¬¡æ•°ï¼‰
            batch_size=32,              # æ‰¹æ¬¡å¤§å°ï¼ˆæ¯æ¬¡å¤„ç†32ä¸ªæ ·æœ¬ï¼‰
            validation_data=(X_test, y_test),  # éªŒè¯æ•°æ®ï¼ˆç”¨äºç›‘æ§è®­ç»ƒæ•ˆæœï¼‰
            verbose=1                   # æ˜¾ç¤ºè®­ç»ƒè¿›åº¦
        )

        """
        è®­ç»ƒè¿‡ç¨‹è§£é‡Šï¼š

        Epochï¼ˆè½®æ¬¡ï¼‰: æ¨¡å‹å®Œæ•´çœ‹ä¸€éæ‰€æœ‰è®­ç»ƒæ•°æ®ç®—ä¸€è½®
        - ç±»ä¼¼äºå­¦ç”Ÿå®Œæ•´å¤ä¹ ä¸€éæ‰€æœ‰è¯¾æœ¬

        Batchï¼ˆæ‰¹æ¬¡ï¼‰: æ¯æ¬¡å¤„ç†çš„æ ·æœ¬æ•°é‡
        - ç±»ä¼¼äºå­¦ç”Ÿæ¯æ¬¡åš32é“é¢˜ï¼Œè€Œä¸æ˜¯ä¸€æ¬¡åšå®Œæ‰€æœ‰é¢˜

        Lossï¼ˆæŸå¤±ï¼‰: æ¨¡å‹é¢„æµ‹é”™è¯¯çš„ç¨‹åº¦
        - æ•°å€¼è¶Šå°è¡¨ç¤ºé¢„æµ‹è¶Šå‡†ç¡®
        - ç±»ä¼¼äºè€ƒè¯•çš„é”™è¯¯ç‡

        Accuracyï¼ˆå‡†ç¡®ç‡ï¼‰: é¢„æµ‹æ­£ç¡®çš„æ¯”ä¾‹
        - æ•°å€¼è¶Šå¤§è¡¨ç¤ºæ¨¡å‹è¶Šå¥½
        - ç±»ä¼¼äºè€ƒè¯•çš„æ­£ç¡®ç‡

        Validationï¼ˆéªŒè¯ï¼‰: ç”¨æµ‹è¯•æ•°æ®æ£€æŸ¥æ¨¡å‹æ˜¯å¦è¿‡æ‹Ÿåˆ
        - ç±»ä¼¼äºç”¨æ¨¡æ‹Ÿè€ƒè¯•æ£€æŸ¥å­¦ä¹ æ•ˆæœ
        """

        # è¯„ä¼°æ¨¡å‹æ€§èƒ½
        test_loss, test_accuracy = self.model.evaluate(X_test, y_test, verbose=0)
        print(f"\nğŸ“Š æ¨¡å‹è¯„ä¼°ç»“æœ:")
        print(f"æµ‹è¯•å‡†ç¡®ç‡: {test_accuracy:.4f}")  # .4fè¡¨ç¤ºä¿ç•™4ä½å°æ•°
        print(f"æµ‹è¯•æŸå¤±: {test_loss:.4f}")

        # è¿”å›è®­ç»ƒå†å²å’Œæµ‹è¯•æ•°æ®ï¼ˆç”¨äºåç»­åˆ†æï¼‰
        return history, (X_test, y_test)

    def predict_status(self, sequence_data):
        """
        é¢„æµ‹è¿è¡ŒçŠ¶æ€çš„æ–¹æ³•
        ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹å¯¹æ–°æ•°æ®è¿›è¡Œé¢„æµ‹
        ç±»ä¼¼äºå­¦ç”Ÿå­¦ä¼šçŸ¥è¯†åï¼Œç”¨æ¥è§£ç­”æ–°é¢˜ç›®

        å‚æ•°:
        sequence_data: è¾“å…¥çš„æ—¶åºæ•°æ®ï¼ˆéœ€è¦é¢„æµ‹çš„åºåˆ—ï¼‰

        è¿”å›:
        predicted_statuses: é¢„æµ‹çš„çŠ¶æ€åç§°ï¼ˆå¦‚"ä½é€Ÿè¿è¡Œ"ï¼‰
        predictions: åŸå§‹é¢„æµ‹æ¦‚ç‡ï¼ˆæ¯ä¸ªç±»åˆ«çš„æ¦‚ç‡å€¼ï¼‰
        """
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²è®­ç»ƒ
        if self.model is None:
            print("âŒ æ¨¡å‹æœªè®­ç»ƒï¼Œè¯·å…ˆè®­ç»ƒæ¨¡å‹")
            return None

        # é¢„å¤„ç†è¾“å…¥æ•°æ®
        # ç¡®ä¿è¾“å…¥æ•°æ®çš„å½¢çŠ¶æ­£ç¡®ï¼ˆéœ€è¦æ˜¯ä¸‰ç»´ï¼šæ‰¹æ¬¡ x æ—¶é—´æ­¥ x ç‰¹å¾ï¼‰
        if len(sequence_data.shape) == 2:
            # å¦‚æœæ˜¯äºŒç»´æ•°æ®ï¼Œæ·»åŠ æ‰¹æ¬¡ç»´åº¦
            # ä¾‹å¦‚ï¼š(10, 15) -> (1, 10, 15)
            sequence_data = sequence_data.reshape(1, *sequence_data.shape)

        # è¿›è¡Œé¢„æµ‹
        # model.predict() è¿”å›æ¯ä¸ªç±»åˆ«çš„æ¦‚ç‡
        predictions = self.model.predict(sequence_data, verbose=0)

        # è·å–æ¦‚ç‡æœ€é«˜çš„ç±»åˆ«ç´¢å¼•
        # np.argmax() æ‰¾åˆ°æœ€å¤§å€¼çš„ç´¢å¼•
        # ä¾‹å¦‚ï¼š[0.1, 0.7, 0.2] -> 1ï¼ˆç¬¬äºŒä¸ªå…ƒç´ æœ€å¤§ï¼‰
        predicted_classes = np.argmax(predictions, axis=1)

        # å°†æ•°å­—æ ‡ç­¾è½¬æ¢å›æ–‡å­—æ ‡ç­¾
        # ä½¿ç”¨ä¹‹å‰ä¿å­˜çš„ç¼–ç å™¨è¿›è¡Œåå‘è½¬æ¢
        # ä¾‹å¦‚ï¼š1 -> "ä½é€Ÿè¿è¡Œ"
        le_target = self.label_encoders['target']
        predicted_statuses = le_target.inverse_transform(predicted_classes)

        """
        é¢„æµ‹ç»“æœè§£é‡Šï¼š

        predictions: åŸå§‹æ¦‚ç‡è¾“å‡º
        ä¾‹å¦‚ï¼š[[0.1, 0.7, 0.15, 0.05]]
        è¡¨ç¤ºï¼š
        - åˆå§‹åŒ–çŠ¶æ€: 10%æ¦‚ç‡
        - ä½é€Ÿè¿è¡Œ: 70%æ¦‚ç‡  <- æœ€é«˜æ¦‚ç‡
        - ä¸­é€Ÿè¿è¡Œ: 15%æ¦‚ç‡
        - é«˜é€Ÿè¿è¡Œ: 5%æ¦‚ç‡

        predicted_classes: [1]  (æ¦‚ç‡æœ€é«˜çš„ç±»åˆ«ç´¢å¼•)
        predicted_statuses: ["ä½é€Ÿè¿è¡Œ"]  (è½¬æ¢åçš„çŠ¶æ€åç§°)
        """

        return predicted_statuses, predictions

    def visualize_results(self, history, test_data=None):
        """å¯è§†åŒ–ç»“æœ"""
        print("\nğŸ“ˆ æ­£åœ¨ç”Ÿæˆå¯è§†åŒ–ç»“æœ...")

        plt.figure(figsize=(15, 10))

        # è®­ç»ƒå†å²
        plt.subplot(2, 3, 1)
        plt.plot(history.history['accuracy'], label='è®­ç»ƒå‡†ç¡®ç‡')
        plt.plot(history.history['val_accuracy'], label='éªŒè¯å‡†ç¡®ç‡')
        plt.title('æ¨¡å‹å‡†ç¡®ç‡')
        plt.xlabel('è®­ç»ƒè½®æ•°')
        plt.ylabel('å‡†ç¡®ç‡')
        plt.legend()
        plt.grid(True)

        plt.subplot(2, 3, 2)
        plt.plot(history.history['loss'], label='è®­ç»ƒæŸå¤±')
        plt.plot(history.history['val_loss'], label='éªŒè¯æŸå¤±')
        plt.title('æ¨¡å‹æŸå¤±')
        plt.xlabel('è®­ç»ƒè½®æ•°')
        plt.ylabel('æŸå¤±å€¼')
        plt.legend()
        plt.grid(True)

        # çŠ¶æ€åˆ†å¸ƒ
        plt.subplot(2, 3, 3)
        status_counts = self.df['è¿è¡ŒçŠ¶æ€'].value_counts()
        plt.pie(status_counts.values, labels=status_counts.index, autopct='%1.1f%%')
        plt.title('è¿è¡ŒçŠ¶æ€åˆ†å¸ƒ')

        # æ—¶åºç‰¹å¾åˆ†æ
        plt.subplot(2, 3, 4)
        if 'é€Ÿåº¦' in self.df.columns:
            plt.plot(self.df['é€Ÿåº¦'][:200], label='é€Ÿåº¦')
            plt.title('é€Ÿåº¦å˜åŒ–è¶‹åŠ¿ï¼ˆå‰200æ¡è®°å½•ï¼‰')
            plt.xlabel('æ—¶é—´åºåˆ—')
            plt.ylabel('é€Ÿåº¦ (km/h)')
            plt.legend()
            plt.grid(True)

        plt.subplot(2, 3, 5)
        if 'ç®¡å‹' in self.df.columns:
            plt.plot(self.df['ç®¡å‹'][:200], label='ç®¡å‹', color='orange')
            plt.title('ç®¡å‹å˜åŒ–è¶‹åŠ¿ï¼ˆå‰200æ¡è®°å½•ï¼‰')
            plt.xlabel('æ—¶é—´åºåˆ—')
            plt.ylabel('ç®¡å‹')
            plt.legend()
            plt.grid(True)

        # æ··æ·†çŸ©é˜µ
        if test_data is not None:
            X_test, y_test = test_data
            y_pred = np.argmax(self.model.predict(X_test, verbose=0), axis=1)

            plt.subplot(2, 3, 6)
            from sklearn.metrics import confusion_matrix
            cm = confusion_matrix(y_test, y_pred)
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
            plt.title('æ··æ·†çŸ©é˜µ')
            plt.xlabel('é¢„æµ‹æ ‡ç­¾')
            plt.ylabel('çœŸå®æ ‡ç­¾')

        plt.tight_layout()
        plt.show()

    def save_model(self, model_path='train_status_model.h5'):
        """ä¿å­˜æ¨¡å‹"""
        if self.model is not None:
            self.model.save(model_path)
            print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}")
        else:
            print("âŒ æ²¡æœ‰å¯ä¿å­˜çš„æ¨¡å‹")

    def load_model(self, model_path='train_status_model.h5'):
        """åŠ è½½æ¨¡å‹"""
        try:
            self.model = tf.keras.models.load_model(model_path)
            print(f"âœ… æ¨¡å‹å·²ä» {model_path} åŠ è½½")
        except:
            print(f"âŒ æ— æ³•åŠ è½½æ¨¡å‹: {model_path}")


def main():
    """
    ä¸»å‡½æ•° - ç¨‹åºçš„å…¥å£ç‚¹
    è¿™ä¸ªå‡½æ•°åè°ƒæ•´ä¸ªæœºå™¨å­¦ä¹ æµç¨‹ï¼Œç±»ä¼¼äºé¡¹ç›®çš„ä¸»æ§åˆ¶å™¨
    æŒ‰ç…§æ ‡å‡†çš„æœºå™¨å­¦ä¹ æµç¨‹æ‰§è¡Œï¼šæ•°æ®å¤„ç† -> ç‰¹å¾å·¥ç¨‹ -> æ¨¡å‹è®­ç»ƒ -> è¯„ä¼° -> é¢„æµ‹
    """
    print("ğŸš€ å¼€å§‹ç«è½¦è¿è¡ŒçŠ¶æ€åˆ†æé¡¹ç›®")
    print("è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„æ·±åº¦å­¦ä¹ é¡¹ç›®ï¼ŒåŒ…å«ä»¥ä¸‹æ­¥éª¤ï¼š")
    print("1. æ•°æ®åŠ è½½å’Œæ¸…æ´—")
    print("2. ç‰¹å¾å·¥ç¨‹")
    print("3. æ—¶åºæ•°æ®å‡†å¤‡")
    print("4. æ¨¡å‹æ„å»ºå’Œè®­ç»ƒ")
    print("5. ç»“æœå¯è§†åŒ–")
    print("6. æ¨¡å‹ä¿å­˜å’Œé¢„æµ‹")

    # ç¬¬ä¸€æ­¥ï¼šåˆ›å»ºåˆ†æå™¨å®ä¾‹
    # ç±»ä¼¼äºåˆ›å»ºä¸€ä¸ªé¡¹ç›®ç®¡ç†å™¨ï¼Œè´Ÿè´£æ•´ä¸ªåˆ†ææµç¨‹
    analyzer = TrainStatusAnalyzer('test.log')

    # ç¬¬äºŒæ­¥ï¼šæ•°æ®å¤„ç†æµç¨‹
    print("\n" + "="*50)
    print("ğŸ“Š æ•°æ®å¤„ç†é˜¶æ®µ")
    print("="*50)

    # åŠ è½½å’Œæ¸…æ´—åŸå§‹æ•°æ®
    analyzer.load_and_clean_data()

    # ç‰¹å¾å·¥ç¨‹ï¼šå°†åŸå§‹æ•°æ®è½¬æ¢ä¸ºæœºå™¨å­¦ä¹ å¯ç”¨çš„ç‰¹å¾
    analyzer.feature_engineering()

    # ç¬¬ä¸‰æ­¥ï¼šå‡†å¤‡æ—¶åºæ•°æ®
    print("\n" + "="*50)
    print("ğŸ”„ æ—¶åºæ•°æ®å‡†å¤‡é˜¶æ®µ")
    print("="*50)

    # å°†è¡¨æ ¼æ•°æ®è½¬æ¢ä¸ºæ—¶åºåºåˆ—æ•°æ®
    # sequence_length=10 è¡¨ç¤ºç”¨è¿‡å»10ä¸ªæ—¶é—´ç‚¹é¢„æµ‹ä¸‹ä¸€ä¸ªæ—¶é—´ç‚¹
    X, y = analyzer.prepare_sequences(sequence_length=10)

    # ç¬¬å››æ­¥ï¼šæ¨¡å‹è®­ç»ƒ
    print("\n" + "="*50)
    print("ğŸ§  æ¨¡å‹è®­ç»ƒé˜¶æ®µ")
    print("="*50)

    # è®­ç»ƒLSTMæ·±åº¦å­¦ä¹ æ¨¡å‹
    # epochs=30 è¡¨ç¤ºæ¨¡å‹çœ‹æ•°æ®30éï¼ˆå¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´ï¼‰
    history, test_data = analyzer.train_model(X, y, epochs=30)

    # ç¬¬äº”æ­¥ï¼šç»“æœå¯è§†åŒ–
    print("\n" + "="*50)
    print("ğŸ“ˆ ç»“æœåˆ†æé˜¶æ®µ")
    print("="*50)

    # ç”Ÿæˆè®­ç»ƒè¿‡ç¨‹å›¾è¡¨å’Œæ€§èƒ½åˆ†æå›¾
    analyzer.visualize_results(history, test_data)

    # ç¬¬å…­æ­¥ï¼šä¿å­˜æ¨¡å‹
    print("\n" + "="*50)
    print("ğŸ’¾ æ¨¡å‹ä¿å­˜é˜¶æ®µ")
    print("="*50)

    # ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œä»¥ä¾¿åç»­ä½¿ç”¨
    analyzer.save_model()

    # ç¬¬ä¸ƒæ­¥ï¼šç¤ºä¾‹é¢„æµ‹
    print("\n" + "="*50)
    print("ğŸ”® é¢„æµ‹æ¼”ç¤ºé˜¶æ®µ")
    print("="*50)

    print("è¿›è¡ŒçŠ¶æ€é¢„æµ‹ç¤ºä¾‹...")

    # å–ç¬¬ä¸€ä¸ªåºåˆ—ä½œä¸ºé¢„æµ‹ç¤ºä¾‹
    sample_sequence = X[0:1]  # å½¢çŠ¶ï¼š(1, 10, ç‰¹å¾æ•°)

    # ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹
    predicted_status, confidence = analyzer.predict_status(sample_sequence)

    print(f"é¢„æµ‹çŠ¶æ€: {predicted_status[0]}")
    print(f"å„çŠ¶æ€ç½®ä¿¡åº¦åˆ†å¸ƒ:")

    # æ˜¾ç¤ºæ¯ä¸ªçŠ¶æ€çš„é¢„æµ‹æ¦‚ç‡
    le_target = analyzer.label_encoders['target']
    for i, prob in enumerate(confidence[0]):
        status_name = le_target.inverse_transform([i])[0]
        print(f"  {status_name}: {prob:.3f} ({prob*100:.1f}%)")

    # é¡¹ç›®å®Œæˆæ€»ç»“
    print("\n" + "=" * 60)
    print("âœ… ç«è½¦è¿è¡ŒçŠ¶æ€åˆ†æé¡¹ç›®å®Œæˆï¼")
    print("=" * 60)
    print("ğŸ“‹ é¡¹ç›®æˆæœ:")
    print("1. âœ… æˆåŠŸå¤„ç†äº†ç«è½¦è¿è¡Œæ—¥å¿—æ•°æ®")
    print("2. âœ… æ„å»ºäº†LSTMæ—¶åºé¢„æµ‹æ¨¡å‹")
    print("3. âœ… å®ç°äº†è¿è¡ŒçŠ¶æ€çš„è‡ªåŠ¨åˆ†ç±»")
    print("4. âœ… ç”Ÿæˆäº†è¯¦ç»†çš„åˆ†ææŠ¥å‘Šå’Œå›¾è¡¨")
    print("5. âœ… ä¿å­˜äº†å¯é‡ç”¨çš„é¢„æµ‹æ¨¡å‹")
    print("\nğŸ’¡ ä¸‹ä¸€æ­¥å¯ä»¥:")
    print("- ä½¿ç”¨æ›´å¤šå†å²æ•°æ®æé«˜æ¨¡å‹å‡†ç¡®æ€§")
    print("- è°ƒæ•´æ¨¡å‹å‚æ•°ä¼˜åŒ–æ€§èƒ½")
    print("- éƒ¨ç½²æ¨¡å‹åˆ°ç”Ÿäº§ç¯å¢ƒè¿›è¡Œå®æ—¶é¢„æµ‹")
    print("=" * 60)


# Pythonç¨‹åºå…¥å£ç‚¹
# å½“ç›´æ¥è¿è¡Œè¿™ä¸ªæ–‡ä»¶æ—¶ï¼Œä¼šæ‰§è¡Œmain()å‡½æ•°
# ç±»ä¼¼äºJavaçš„public static void main()æˆ–Cçš„int main()
if __name__ == "__main__":
    main()
