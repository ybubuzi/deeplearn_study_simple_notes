"""
ç®€åŒ–ç‰ˆç«è½¦çŠ¶æ€åˆ†æ - å¿«é€Ÿå…¥é—¨ç‰ˆæœ¬
ä¸“é—¨ç”¨äºç†è§£æ•°æ®ç»“æ„å’ŒåŸºç¡€æ¨¡å‹è®­ç»ƒ
"""

import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

print("ğŸš‚ ç®€åŒ–ç‰ˆç«è½¦è¿è¡ŒçŠ¶æ€åˆ†æ")
print("=" * 50)

def load_train_data(file_path):
    """åŠ è½½ç«è½¦æ—¥å¿—æ•°æ®"""
    print("ğŸ“Š æ­£åœ¨åŠ è½½æ•°æ®...")
    
    # æ‰‹åŠ¨è§£ææ•°æ®æ–‡ä»¶
    data = []
    with open(file_path, 'r', encoding='utf-8') as f:
        lines = f.readlines()
    
    # è·å–æ ‡é¢˜è¡Œ
    headers = lines[0].strip().split()
    print(f"æ•°æ®åˆ—: {headers}")
    
    # è§£ææ•°æ®è¡Œ
    for i, line in enumerate(lines[1:], 1):
        parts = line.strip().split()
        if len(parts) >= 11:  # ç¡®ä¿æœ‰è¶³å¤Ÿçš„åˆ—
            # æå–ä¸»è¦å­—æ®µ
            try:
                row = {
                    'åºå·': int(parts[0]) if parts[0].isdigit() else i,
                    'è®°å½•åç§°': parts[1],
                    'æ—¥æœŸæ—¶é—´': parts[2],
                    'å…¬é‡Œæ ‡': float(parts[3]) if parts[3].replace('.', '').isdigit() else 0,
                    'è·ç¦»': int(parts[4]) if parts[4].isdigit() else 0,
                    'è½¦ç«™å': parts[5] if len(parts) > 5 else '',
                    'ä¿¡å·ç¯': parts[6] if len(parts) > 6 else '',
                    'é€Ÿåº¦': int(parts[7]) if parts[7].isdigit() else 0,
                    'é™é€Ÿ': int(parts[8]) if parts[8].isdigit() else 0,
                    'ç©ºæŒ¡': parts[9] if len(parts) > 9 else '',
                    'å‰å': parts[10] if len(parts) > 10 else '',
                    'ç®¡å‹': int(parts[11]) if len(parts) > 11 and parts[11].isdigit() else 0,
                    'å…¶ä»–': ' '.join(parts[12:]) if len(parts) > 12 else ''
                }
                data.append(row)
            except:
                continue
    
    df = pd.DataFrame(data)
    print(f"æˆåŠŸåŠ è½½ {len(df)} æ¡è®°å½•")
    return df

def create_simple_features(df):
    """åˆ›å»ºç®€å•ç‰¹å¾"""
    print("âš™ï¸ æ­£åœ¨åˆ›å»ºç‰¹å¾...")
    
    # åˆ›å»ºè¿è¡ŒçŠ¶æ€æ ‡ç­¾ï¼ˆç®€åŒ–ç‰ˆï¼‰
    def get_status(row):
        speed = row['é€Ÿåº¦']
        signal = str(row['ä¿¡å·ç¯'])
        record = str(row['è®°å½•åç§°'])
        
        if 'å¼€æœº' in record or 'è¿›å…¥' in record:
            return 0  # åˆå§‹åŒ–
        elif speed == 0:
            return 1  # åœè½¦
        elif speed > 0 and speed <= 30:
            return 2  # ä½é€Ÿ
        elif speed > 30 and speed <= 60:
            return 3  # ä¸­é€Ÿ
        elif speed > 60:
            return 4  # é«˜é€Ÿ
        else:
            return 5  # å…¶ä»–
    
    df['çŠ¶æ€'] = df.apply(get_status, axis=1)
    
    # çŠ¶æ€åç§°æ˜ å°„
    status_names = {0: 'åˆå§‹åŒ–', 1: 'åœè½¦', 2: 'ä½é€Ÿ', 3: 'ä¸­é€Ÿ', 4: 'é«˜é€Ÿ', 5: 'å…¶ä»–'}
    
    # æ˜¾ç¤ºçŠ¶æ€åˆ†å¸ƒ
    print("çŠ¶æ€åˆ†å¸ƒ:")
    for status, count in df['çŠ¶æ€'].value_counts().sort_index().items():
        print(f"  {status_names[status]}: {count}æ¡")
    
    # é€‰æ‹©æ•°å€¼ç‰¹å¾
    features = ['é€Ÿåº¦', 'é™é€Ÿ', 'ç®¡å‹', 'è·ç¦»', 'å…¬é‡Œæ ‡']
    
    # åˆ›å»ºç‰¹å¾çŸ©é˜µ
    X = df[features].values
    y = df['çŠ¶æ€'].values
    
    print(f"ç‰¹å¾çŸ©é˜µå½¢çŠ¶: {X.shape}")
    print(f"æ ‡ç­¾æ•°ç»„å½¢çŠ¶: {y.shape}")
    
    return X, y, status_names, df

def build_simple_model(input_dim, num_classes):
    """æ„å»ºç®€å•çš„ç¥ç»ç½‘ç»œæ¨¡å‹"""
    print("ğŸ§  æ„å»ºç¥ç»ç½‘ç»œæ¨¡å‹...")
    
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(64, activation='relu', input_shape=(input_dim,), name='hidden1'),
        tf.keras.layers.Dropout(0.3, name='dropout1'),
        tf.keras.layers.Dense(32, activation='relu', name='hidden2'),
        tf.keras.layers.Dropout(0.3, name='dropout2'),
        tf.keras.layers.Dense(num_classes, activation='softmax', name='output')
    ])
    
    model.compile(
        optimizer='adam',
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    
    print("æ¨¡å‹ç»“æ„:")
    model.summary()
    return model

def train_and_evaluate(X, y, status_names):
    """è®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹"""
    print("ğŸš€ å¼€å§‹è®­ç»ƒæ¨¡å‹...")
    
    # æ•°æ®æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # åˆ†å‰²æ•°æ®
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y, test_size=0.2, random_state=42, stratify=y
    )
    
    print(f"è®­ç»ƒé›†: {X_train.shape[0]}æ¡, æµ‹è¯•é›†: {X_test.shape[0]}æ¡")
    
    # æ„å»ºå’Œè®­ç»ƒæ¨¡å‹
    model = build_simple_model(X_train.shape[1], len(status_names))
    
    history = model.fit(
        X_train, y_train,
        epochs=50,
        batch_size=32,
        validation_data=(X_test, y_test),
        verbose=1
    )
    
    # è¯„ä¼°æ¨¡å‹
    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)
    print(f"\nğŸ“Š æœ€ç»ˆç»“æœ:")
    print(f"æµ‹è¯•å‡†ç¡®ç‡: {test_accuracy:.4f}")
    print(f"æµ‹è¯•æŸå¤±: {test_loss:.4f}")
    
    return model, history, scaler, (X_test, y_test)

def visualize_results(df, history, model, test_data, status_names):
    """å¯è§†åŒ–åˆ†æç»“æœ"""
    print("ğŸ“ˆ ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    
    plt.figure(figsize=(15, 10))
    
    # 1. è®­ç»ƒè¿‡ç¨‹
    plt.subplot(2, 3, 1)
    plt.plot(history.history['accuracy'], label='è®­ç»ƒå‡†ç¡®ç‡')
    plt.plot(history.history['val_accuracy'], label='éªŒè¯å‡†ç¡®ç‡')
    plt.title('æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ - å‡†ç¡®ç‡')
    plt.xlabel('è®­ç»ƒè½®æ•°')
    plt.ylabel('å‡†ç¡®ç‡')
    plt.legend()
    plt.grid(True)
    
    plt.subplot(2, 3, 2)
    plt.plot(history.history['loss'], label='è®­ç»ƒæŸå¤±')
    plt.plot(history.history['val_loss'], label='éªŒè¯æŸå¤±')
    plt.title('æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ - æŸå¤±')
    plt.xlabel('è®­ç»ƒè½®æ•°')
    plt.ylabel('æŸå¤±å€¼')
    plt.legend()
    plt.grid(True)
    
    # 2. çŠ¶æ€åˆ†å¸ƒ
    plt.subplot(2, 3, 3)
    status_counts = df['çŠ¶æ€'].value_counts().sort_index()
    status_labels = [status_names[i] for i in status_counts.index]
    plt.pie(status_counts.values, labels=status_labels, autopct='%1.1f%%')
    plt.title('è¿è¡ŒçŠ¶æ€åˆ†å¸ƒ')
    
    # 3. é€Ÿåº¦å˜åŒ–è¶‹åŠ¿
    plt.subplot(2, 3, 4)
    sample_size = min(500, len(df))
    plt.plot(df['é€Ÿåº¦'][:sample_size], label='é€Ÿåº¦', alpha=0.7)
    plt.title(f'é€Ÿåº¦å˜åŒ–è¶‹åŠ¿ï¼ˆå‰{sample_size}æ¡è®°å½•ï¼‰')
    plt.xlabel('æ—¶é—´åºåˆ—')
    plt.ylabel('é€Ÿåº¦ (km/h)')
    plt.legend()
    plt.grid(True)
    
    # 4. ç®¡å‹å˜åŒ–è¶‹åŠ¿
    plt.subplot(2, 3, 5)
    plt.plot(df['ç®¡å‹'][:sample_size], label='ç®¡å‹', color='orange', alpha=0.7)
    plt.title(f'ç®¡å‹å˜åŒ–è¶‹åŠ¿ï¼ˆå‰{sample_size}æ¡è®°å½•ï¼‰')
    plt.xlabel('æ—¶é—´åºåˆ—')
    plt.ylabel('ç®¡å‹')
    plt.legend()
    plt.grid(True)
    
    # 5. é¢„æµ‹ç»“æœåˆ†æ
    plt.subplot(2, 3, 6)
    X_test, y_test = test_data
    y_pred = np.argmax(model.predict(X_test, verbose=0), axis=1)
    
    # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
    from sklearn.metrics import classification_report
    report = classification_report(y_test, y_pred, output_dict=True)
    
    categories = [status_names[i] for i in sorted(status_names.keys()) if str(i) in report]
    accuracies = [report[str(i)]['f1-score'] for i in sorted(status_names.keys()) if str(i) in report]
    
    plt.bar(categories, accuracies)
    plt.title('å„çŠ¶æ€é¢„æµ‹å‡†ç¡®ç‡')
    plt.xlabel('è¿è¡ŒçŠ¶æ€')
    plt.ylabel('F1åˆ†æ•°')
    plt.xticks(rotation=45)
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()

def predict_example(model, scaler, X, y, status_names, num_examples=5):
    """é¢„æµ‹ç¤ºä¾‹"""
    print(f"\nğŸ”® é¢„æµ‹ç¤ºä¾‹ï¼ˆéšæœºé€‰æ‹©{num_examples}ä¸ªæ ·æœ¬ï¼‰:")
    
    # éšæœºé€‰æ‹©æ ·æœ¬
    indices = np.random.choice(len(X), num_examples, replace=False)
    
    for i, idx in enumerate(indices):
        sample = X[idx:idx+1]
        sample_scaled = scaler.transform(sample)
        
        # é¢„æµ‹
        prediction = model.predict(sample_scaled, verbose=0)
        predicted_class = np.argmax(prediction)
        confidence = np.max(prediction) * 100
        
        true_class = y[idx]
        
        print(f"æ ·æœ¬ {i+1}:")
        print(f"  è¾“å…¥ç‰¹å¾: é€Ÿåº¦={sample[0][0]}, é™é€Ÿ={sample[0][1]}, ç®¡å‹={sample[0][2]}")
        print(f"  çœŸå®çŠ¶æ€: {status_names[true_class]}")
        print(f"  é¢„æµ‹çŠ¶æ€: {status_names[predicted_class]}")
        print(f"  é¢„æµ‹ç½®ä¿¡åº¦: {confidence:.1f}%")
        print(f"  é¢„æµ‹{'âœ…æ­£ç¡®' if predicted_class == true_class else 'âŒé”™è¯¯'}")
        print()

def main():
    """ä¸»å‡½æ•°"""
    try:
        # 1. åŠ è½½æ•°æ®
        df = load_train_data('test.log')
        
        # 2. ç‰¹å¾å·¥ç¨‹
        X, y, status_names, df = create_simple_features(df)
        
        # 3. è®­ç»ƒæ¨¡å‹
        model, history, scaler, test_data = train_and_evaluate(X, y, status_names)
        
        # 4. å¯è§†åŒ–ç»“æœ
        visualize_results(df, history, model, test_data, status_names)
        
        # 5. é¢„æµ‹ç¤ºä¾‹
        predict_example(model, scaler, X, y, status_names)
        
        # 6. ä¿å­˜æ¨¡å‹
        model.save('simple_train_model.h5')
        print("âœ… æ¨¡å‹å·²ä¿å­˜ä¸º 'simple_train_model.h5'")
        
        print("\n" + "=" * 50)
        print("ğŸ‰ ç«è½¦çŠ¶æ€åˆ†æå®Œæˆï¼")
        print("=" * 50)
        
    except Exception as e:
        print(f"âŒ è¿è¡Œå‡ºé”™: {e}")
        print("è¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”æ ¼å¼æ­£ç¡®")

if __name__ == "__main__":
    main()
