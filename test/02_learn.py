import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

# è®¾ç½®matplotlibä¸­æ–‡å­—ä½“æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['SimHei']  # è®¾ç½®å­—ä½“ä¸ºé»‘ä½“
plt.rcParams['axes.unicode_minus'] = False  # æ­£å¸¸æ˜¾ç¤ºè´Ÿå·

# åŠ è½½MNISTæ•°æ®é›†
# è¯¥ä»£ç æ˜¯è°ƒç”¨å®˜æ–¹åº“ï¼Œä¸‹è½½å®˜æ–¹çš„æµ‹è¯•è®­ç»ƒé›†ï¼Œå­˜æ”¾è‡³æœ¬åœ°%UserData%/.keras/datasetsä¸‹
# å†å…·ä½“ç‚¹å°±æ˜¯è¯¥ç›®å½•ä¸‹çš„mnist.npzæ–‡ä»¶ï¼Œå¯ä»¥æ›´æ”¹åç¼€åä½¿ç”¨zipæ‰“å¼€æŸ¥çœ‹
(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()
"""
è¯¥å‹ç¼©åŒ…è§£å‹åä¼šæœ‰4ä¸ªæ–‡ä»¶ï¼Œy_train.npyã€x_train.npyã€y_test.npyã€x_test.npy
è®­ç»ƒé›†å›¾å½¢æ˜¯ä¸€ä¸ªç°åº¦å›¾é›†ï¼Œå› æ­¤åƒç´ 28*28ï¼Œæ¯ä¸€ä¸ªåƒç´ ç‚¹ä»…ç”±ä¸€ä¸ªç°åº¦å€¼æ„æˆ
ä¸€èˆ¬çš„å½©è‰²å›¾åƒçš„æ„æˆç”±ä¸‰ä¸ªæ•°å€¼æˆ–å››ä¸ªæ„æˆï¼ŒåŠRã€Gã€Bã€A
çœ‹åå­—å¾—çŸ¥å°±æ˜¯è®­ç»ƒæ•°æ®é›†å’ŒéªŒè¯æ•°æ®é›†
- X_trainï¼šè®­ç»ƒå›¾åƒæ•°æ®ï¼Œå½¢çŠ¶ä¸º(60000, 28, 28)
    xçš„å½¢çŠ¶ä¸º[
                [
                    [1,2,3,4,5,6,7,....,28] # ç¬¬1è¡Œ
                    ...
                    [1,2,3,4,5,6,7,....,28] # ç¬¬28è¡Œ
                ]ï¼Œ
                ... è¿™é‡Œæ€»å…±æœ‰60000ä¸ª
            ]
- y_trainï¼šè®­ç»ƒæ ‡ç­¾æ•°æ®ï¼Œå½¢çŠ¶ä¸º(60000,)ï¼Œå€¼ä¸º0-9
    yçš„å½¢çŠ¶ä¸º: [1,2,3,4...0] è¿™é‡Œä¸€å…±æœ‰60000é•¿åº¦

- X_testï¼šæµ‹è¯•å›¾åƒæ•°æ®ï¼Œå½¢çŠ¶ä¸º(10000, 28, 28)  
- y_testï¼šæµ‹è¯•æ ‡ç­¾æ•°æ®ï¼Œå½¢çŠ¶ä¸º(10000,)
"""
# è¿™é‡Œè·å–äº†æ•´ä¸ªçŸ©é˜µç©ºé—´ä¸­æœ€æ˜äº®çš„åƒç´ ç‚¹å’Œæœ€æš—çš„åƒç´ ç‚¹ï¼Œæ³¨æ„è¯¥å›¾æ˜¯ç°åº¦å›¾ï¼Œä»…æœ‰ç°åº¦ï¼ˆäº®åº¦ï¼‰ä¿¡æ¯
print(f"åƒç´ å€¼èŒƒå›´: {X_train.min()} - {X_train.max()}")

"""
minå’Œmaxæ–¹æ³•è·å–æ•´ä¸ªçŸ©é˜µç©ºé—´ä¸­çš„æå€¼
"""

# æ•°æ®é¢„å¤„ç†
# æ•°æ®å½’ä¸€åŒ–ï¼šå°†åƒç´ å€¼ä»0-255ç¼©æ”¾åˆ°0-1
X_train = X_train.astype('float32') / 255.0
X_test = X_test.astype('float32') / 255.0

"""
å°†æ•°æ®ç­‰æ¯”å‹ç¼©è‡³0~1ï¼›
ç”±äºä¸€ä¸ªåƒç´ ä¿¡æ¯æœ€å¤§ä¸º255ï¼Œè¿™é‡Œåªéœ€é™¤ä»¥255å³å¯ç¼©æ”¾è‡³1
"""

X_train_flat = X_train.reshape(X_train.shape[0], -1)
X_test_flat = X_test.reshape(X_test.shape[0], -1)

"""
å°†å¼ é‡æ‰å¹³åŒ–
X_train.shapeæœ¬èº«æ˜¯ä¸€ä¸ªå…ƒç»„(60000,28,28),
æ‰€ä»¥è¿™é‡Œçš„X_train.shape[0]ï¼Œå…¶å®å°±æ˜¯60000ï¼Œ
è¿™é‡Œçš„æ„æ€æ˜¯ä¿ç•™60000ä¸ªå…ƒç´ ï¼Œè€Œæ¯ä¸ªå…ƒç´ å†…çš„å­å…ƒç´ æ•°é‡ç”±npè‡ªåŠ¨è®¡ç®—ï¼Œè¿™é‡Œå°±æ˜¯28*28æ¥å¡«å……äº†
æœ€ç»ˆå°±å˜æˆäº†
x: [
        [255,255,255,255,255,......,255] # ç¬¬ä¸€å¼ å›¾çš„ä¿¡æ¯ï¼Œæ€»å…±28*28 = 784ä¸ªå…ƒç´ 
        ...
        ... 
        [255,255,255,255,255,......,255] # è¿™æ˜¯ç¬¬60000å¼ å›¾çš„ä¿¡æ¯ï¼Œä¹Ÿæ˜¯784ä¸ªå…ƒç´ 
    ]
"""

plt.figure(figsize=(12, 4))

for i in range(10):
    plt.subplot(2, 5, i + 1)
    plt.imshow(X_train[i], cmap='gray')
    plt.title(f'æ ‡ç­¾: {y_train[i]}')  # è®¾ç½®å­å›¾æ ‡é¢˜
    plt.axis('off')  # éšè—åæ ‡è½´
plt.suptitle('MNISTæ•°æ®é›†æ ·æœ¬')  # è®¾ç½®æ•´ä¸ªå›¾å½¢çš„æ ‡é¢˜
plt.tight_layout()  # è‡ªåŠ¨è°ƒæ•´å­å›¾é—´è·
plt.show()  # æ˜¾ç¤ºå›¾å½¢
"""
è¿™é‡Œåˆ›å»ºäº†ä¸€ä¸ª2è¡Œ5åˆ—çš„çª—å£ï¼Œæ¯ä¸€ä¸ªå•å…ƒæ ¼æ˜¾ç¤ºä¸€å¼ å›¾ç‰‡
"""

model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu', input_shape=(784,), name='hidden_layer1'),
    tf.keras.layers.Dropout(0.2, name='dropout_layer'),
    tf.keras.layers.Dense(64, activation='relu', name='hidden_layer2'),
    tf.keras.layers.Dense(10, activation='softmax', name='output_layer')  # æ·»åŠ è¾“å‡ºå±‚
])
"""
ç¬¬ä¸€ä¸ªéšè—å±‚ï¼š
è¿™é‡Œçš„128è¡¨ç¤ºæœ‰128ä¸ªç¥ç»å…ƒï¼Œæ¯ä¸€ä¸ªç¥ç»å…ƒå¯¹ä¸€ä¸ªå›¾ç‰‡éƒ½ä¼šæœ‰å”¯ä¸€çš„ç‰¹å¾è¾“å‡º
æœ€ç»ˆè¾“å‡º128ä¸ªç‰¹å¾
activation='relu'ï¼šæ˜¯æ¿€æ´»å‡½æ•°ï¼Œå¯¹ç¥ç»å…ƒçš„è¾“å‡ºå€¼æœ€å°ä¸º0.ä¼ªä»£ç ä¸º max(0,x)
input=(784,)è¡¨ç¤ºä¸Šä¸€å±‚çš„è¾“å…¥å½¢çŠ¶ï¼Œå› ä¸ºåœ¨æ­¤ä¹‹å‰æˆ‘ä»¬å¯¹æ¯ä¸€å¼ å›¾ç‰‡è¿›è¡Œäº†æ‰å¹³åŒ–ï¼Œä¸€å¼ å›¾ç‰‡å°±æ˜¯ç”±784ä¸ªæ•°å­—æ„æˆçš„ä¸€ç»´æ•°å€¼

è¿™é‡Œçš„128ä¸ªç¥ç»å…ƒéƒ½ä¼šè¾“å…¥å®Œæ•´çš„784ä¸ªåƒç´ ç‚¹
è¾“å…¥å±‚(784ä¸ªåƒç´ )    éšè—å±‚1(128ä¸ªç¥ç»å…ƒ)
     â†“                    â†“
   åƒç´ 1  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ ç¥ç»å…ƒ1
   åƒç´ 2  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ ç¥ç»å…ƒ1  
   åƒç´ 3  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ ç¥ç»å…ƒ1
   ...    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ ç¥ç»å…ƒ1
   åƒç´ 784 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ ç¥ç»å…ƒ1

   åƒç´ 1  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ ç¥ç»å…ƒ2
   åƒç´ 2  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ ç¥ç»å…ƒ2
   ...    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ ç¥ç»å…ƒ2
   åƒç´ 784 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ ç¥ç»å…ƒ2

   ... (æ¯ä¸ªç¥ç»å…ƒéƒ½è¿æ¥æ‰€æœ‰784ä¸ªåƒç´ )

Dropoutå±‚ç”¨äºéšæœºå…³é—­æŸäº›ç¥ç»å…ƒï¼Œå°±æ˜¯è®©è¾“å‡ºç‰¹å¾å‡å°‘ï¼›é˜²æ­¢æ¨¡å‹ä¾èµ–æŸä¸€å›ºå®šçš„ç‰¹å¾è€Œäº§ç”Ÿè¿‡æ‹Ÿåˆï¼›
è¯¥å±‚åªä¼šåœ¨è®­ç»ƒæ—¶ç”Ÿæ•ˆ
è¿‡æ‹Ÿåˆé—®é¢˜ï¼š
    - æ¨¡å‹åœ¨è®­ç»ƒæ•°æ®ä¸Šè¡¨ç°å¾ˆå¥½ï¼Œä½†åœ¨æ–°æ•°æ®ä¸Šè¡¨ç°å·®
    - ç±»ä¼¼äºæ­»è®°ç¡¬èƒŒï¼Œä¸èƒ½ä¸¾ä¸€åä¸‰
    - Dropouté€šè¿‡å¢åŠ éšæœºæ€§æ¥ç¼“è§£è¿™ä¸ªé—®é¢˜
ä¹Ÿå°±æ˜¯è¯´ç»è¿‡è¯¥å±‚çš„128ä¸ªç‰¹å¾ç‚¹ï¼Œä¼šéšæœºä¸¢å¤±20%ï¼›
ç¬¬äºŒä¸ªéšè—å±‚ï¼š
è¿™ä¸€å±‚åªæœ‰64ä¸ªç¥ç»å…ƒï¼Œä¹Ÿå°±æ˜¯ä¼šè¾“å‡º64ä¸ªç‰¹å¾ç‚¹ï¼›
è¿™é‡Œçš„è¾“å…¥æ˜¯ä¸Šä¸€ä¸ªéšè—å±‚çš„è¾“å‡ºï¼Œä¹Ÿå°±æ˜¯ä¸Šä¸€å±‚è¾“å‡ºçš„128ä¸ªç‰¹å¾ç‚¹ï¼Œç»è¿‡dropoutååœ¨ä¸è¯¥å±‚ç¥ç»ç‚¹è¿›è¡Œé“¾æ¥ï¼›
è¿™é‡Œæ¯ä¸€ä¸ªç¥ç»å…ƒéƒ½ä¼šå§128ä¸ªç‰¹å¾â€œçœ‹â€ä¸€éï¼Œå¹¶

æœ€åçš„æ˜¯è¾“å‡ºå±‚
è¾“å‡ºå±‚æœ‰10ä¸ªè¡¨ç¤ºæœ‰10ä¸ªçŠ¶æ€ï¼Œæ­£å¥½å¯¹åº”0~9çš„æ•°å­—
activation='softmax'ï¼š
       - Softmaxæ¿€æ´»å‡½æ•°ï¼Œå°†è¾“å‡ºè½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒ
       - æ‰€æœ‰è¾“å‡ºæ¦‚ç‡åŠ èµ·æ¥ç­‰äº1
       - ç±»ä¼¼äºæŠ•ç¥¨ç»“æœçš„ç™¾åˆ†æ¯”
ä¾‹å¦‚è¾“å‡ºï¼š[0.1, 0.05, 0.8, 0.02, 0.01, 0.01, 0.005, 0.005, 0.005, 0.005]
    è¡¨ç¤ºï¼šæ•°å­—2çš„æ¦‚ç‡æ˜¯80%ï¼Œæ•°å­—0çš„æ¦‚ç‡æ˜¯10%ï¼Œç­‰ç­‰ï¼Œè¿™é‡Œçš„2æ˜¯æŒ‡è¯¥è¾“å‡ºå±‚çš„ç¬¬ä¸‰ä¸ªç¥ç»å…ƒï¼Œä¹Ÿå°±æ˜¯ä¸‹æ ‡ä¸º2çš„å€¼
"""

# ============================================================================
# ğŸ§  ç¥ç»ç½‘ç»œè¯¦ç»†è§£æ
# ============================================================================

"""
ğŸ“Š é‡è¦ç†è§£çº æ­£ï¼šæ•°æ®æµåŠ¨è¿‡ç¨‹
âŒ é”™è¯¯ç†è§£ï¼š784ä¸ªåƒç´ ä¾æ¬¡ä¼ é€’ï¼Œä¸€å¼ å›¾ç‰‡è®¡ç®—784æ¬¡
âœ… æ­£ç¡®ç†è§£ï¼šæ‰€æœ‰784ä¸ªåƒç´ åŒæ—¶å¹¶è¡Œä¼ é€’ï¼Œä¸€å¼ å›¾ç‰‡åªè®¡ç®—ä¸€æ¬¡ï¼

ğŸ”„ ä¸€å¼ å›¾ç‰‡çš„å®Œæ•´æ•°æ®æµåŠ¨ï¼š
è¾“å…¥: [784ä¸ªåƒç´ ] â†’ éšè—å±‚1: [128ä¸ªç‰¹å¾] â†’ Dropout â†’ éšè—å±‚2: [64ä¸ªç‰¹å¾] â†’ è¾“å‡º: [10ä¸ªæ¦‚ç‡]
å½¢çŠ¶: (1,784)     â†’        (1,128)        â†’        â†’       (1,64)       â†’      (1,10)

ğŸ§® ç¥ç»å…ƒè®¡ç®—å…¬å¼è¯¦è§£ï¼ˆå…³é”®ç†è§£ï¼‰ï¼š

å¯¹äºç¬¬ä¸€å±‚çš„ç¥ç»å…ƒiï¼ˆi=1åˆ°128ï¼‰ï¼š
ç¥ç»å…ƒiçš„è¾“å‡º = ReLU(p1Ã—wi1 + p2Ã—wi2 + p3Ã—wi3 + ... + p784Ã—wi784 + bi)

å…¶ä¸­ï¼š
- p1, p2, ..., p784ï¼š784ä¸ªåƒç´ å€¼ï¼ˆ0-1ä¹‹é—´çš„å°æ•°ï¼‰
- wi1, wi2, ..., wi784ï¼šç¥ç»å…ƒiå¯¹åº”çš„784ä¸ªæƒé‡ï¼ˆæ¯ä¸ªåƒç´ éƒ½æœ‰ä¸åŒçš„æƒé‡ï¼ï¼‰
- biï¼šç¥ç»å…ƒiçš„åç½®
- ReLUï¼šæ¿€æ´»å‡½æ•° max(0,x)

âŒ æˆ‘ä¹‹å‰çš„é”™è¯¯ç†è§£ï¼šp1Ã—w + p2Ã—w + p3Ã—w + ... + p784Ã—w + bï¼ˆæ‰€æœ‰åƒç´ ç”¨ç›¸åŒæƒé‡ï¼‰
âœ… æ­£ç¡®ç†è§£ï¼šp1Ã—w1 + p2Ã—w2 + p3Ã—w3 + ... + p784Ã—w784 + bï¼ˆæ¯ä¸ªåƒç´ æœ‰ä¸åŒæƒé‡ï¼‰

ğŸ¯ ä¸ºä»€ä¹ˆæ¯ä¸ªåƒç´ éœ€è¦ä¸åŒæƒé‡ï¼Ÿ
- æƒé‡å†³å®šäº†è¯¥åƒç´ å¯¹ç¥ç»å…ƒè¾“å‡ºçš„é‡è¦æ€§
- æ­£æƒé‡ï¼šè¯¥åƒç´ äº®æ—¶å¢åŠ è¾“å‡º
- è´Ÿæƒé‡ï¼šè¯¥åƒç´ äº®æ—¶å‡å°‘è¾“å‡º
- é›¶æƒé‡ï¼šè¯¥åƒç´ è¢«å¿½ç•¥
- ä¸åŒç¥ç»å…ƒé€šè¿‡ä¸åŒçš„æƒé‡ç»„åˆæ¥æ£€æµ‹ä¸åŒçš„ç‰¹å¾

ğŸ’¾ æƒé‡çŸ©é˜µç»“æ„è¯¦è§£ï¼š
ç¬¬ä¸€å±‚æƒé‡çŸ©é˜µå½¢çŠ¶ï¼š(784, 128)
        ç¥ç»å…ƒ1  ç¥ç»å…ƒ2  ç¥ç»å…ƒ3  ...  ç¥ç»å…ƒ128
åƒç´ 1   [  w1,1    w1,2    w1,3   ...   w1,128  ]
åƒç´ 2   [  w2,1    w2,2    w2,3   ...   w2,128  ]
åƒç´ 3   [  w3,1    w3,2    w3,3   ...   w3,128  ]
...     [   ...     ...     ...   ...     ...   ]
åƒç´ 784 [ w784,1  w784,2  w784,3  ...  w784,128 ]

æ€»å‚æ•°ï¼š784Ã—128 + 128 = 100,480ä¸ªå‚æ•°ï¼ˆæƒé‡+åç½®ï¼‰

ğŸ¯ ç¥ç»å…ƒçš„ä½œç”¨ï¼ˆç‰¹å¾æ£€æµ‹å™¨ï¼‰ï¼š
- ç¥ç»å…ƒ1ï¼šå¯èƒ½ä¸“é—¨æ£€æµ‹æ°´å¹³è¾¹ç¼˜ç‰¹å¾
- ç¥ç»å…ƒ2ï¼šå¯èƒ½ä¸“é—¨æ£€æµ‹å‚ç›´è¾¹ç¼˜ç‰¹å¾
- ç¥ç»å…ƒ3ï¼šå¯èƒ½ä¸“é—¨æ£€æµ‹åœ†å½¢ç‰¹å¾
- ç¥ç»å…ƒ4ï¼šå¯èƒ½ä¸“é—¨æ£€æµ‹è§’ç‚¹ç‰¹å¾
- ...
- ç¥ç»å…ƒ128ï¼šå¯èƒ½ä¸“é—¨æ£€æµ‹å…¶ä»–ç‰¹å®šç‰¹å¾
- æ¯ä¸ªç¥ç»å…ƒé€šè¿‡ä¸åŒçš„æƒé‡ç»„åˆæ¥"å…³æ³¨"å›¾åƒçš„ä¸åŒæ–¹é¢

ğŸ” ç¬¬äºŒå±‚ç¥ç»å…ƒè®¡ç®—ï¼š
å¯¹äºç¬¬äºŒå±‚çš„ç¥ç»å…ƒjï¼ˆj=1åˆ°64ï¼‰ï¼š
ç¥ç»å…ƒjçš„è¾“å‡º = ReLU(f1Ã—wj1 + f2Ã—wj2 + ... + f128Ã—wj128 + bj)

å…¶ä¸­ï¼š
- f1, f2, ..., f128ï¼šæ¥è‡ªç¬¬ä¸€å±‚çš„128ä¸ªç‰¹å¾å€¼
- wj1, wj2, ..., wj128ï¼šç¥ç»å…ƒjå¯¹åº”çš„128ä¸ªæƒé‡
- bjï¼šç¥ç»å…ƒjçš„åç½®

ğŸ¯ è¾“å‡ºå±‚è®¡ç®—ï¼š
å¯¹äºè¾“å‡ºå±‚çš„ç¥ç»å…ƒkï¼ˆk=1åˆ°10ï¼Œå¯¹åº”æ•°å­—0-9ï¼‰ï¼š
ç¥ç»å…ƒkçš„è¾“å‡º = Softmax(g1Ã—wk1 + g2Ã—wk2 + ... + g64Ã—wk64 + bk)

å…¶ä¸­ï¼š
- g1, g2, ..., g64ï¼šæ¥è‡ªç¬¬äºŒå±‚çš„64ä¸ªç‰¹å¾å€¼
- Softmaxï¼šå°†è¾“å‡ºè½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒï¼Œæ‰€æœ‰è¾“å‡ºå’Œä¸º1

ğŸ“ˆ æŸå¤±è®¡ç®—è¯¦è§£ï¼š
1. æ¨¡å‹è¾“å‡ºï¼š10ä¸ªæ¦‚ç‡å€¼ï¼Œå¦‚ [0.1, 0.05, 0.8, 0.02, 0.01, 0.01, 0.005, 0.005, 0.005, 0.005]
2. çœŸå®æ ‡ç­¾ï¼šå¦‚ 2ï¼ˆè¡¨ç¤ºè¿™å¼ å›¾ç‰‡æ˜¯æ•°å­—2ï¼‰
3. äº¤å‰ç†µæŸå¤±ï¼šæŸå¤± = -log(é¢„æµ‹æ¦‚ç‡[çœŸå®ç±»åˆ«]) = -log(0.8) = 0.223
4. æ‰¹æ¬¡æŸå¤±ï¼šæ‰€æœ‰æ ·æœ¬æŸå¤±çš„å¹³å‡å€¼

âš¡ è®¡ç®—å¤æ‚åº¦åˆ†æï¼š
å•å¼ å›¾ç‰‡çš„è®¡ç®—æ¬¡æ•°ï¼š
â€¢ ç¬¬1å±‚ï¼š784 Ã— 128 = 100,352 æ¬¡ä¹˜æ³•
â€¢ ç¬¬2å±‚ï¼š128 Ã— 64 = 8,192 æ¬¡ä¹˜æ³•
â€¢ è¾“å‡ºå±‚ï¼š64 Ã— 10 = 640 æ¬¡ä¹˜æ³•
â€¢ æ€»è®¡ï¼šçº¦ 109,184 æ¬¡ä¹˜æ³•è¿ç®—

ğŸ’¡ å…³é”®ç†è§£ï¼š
- è¿™äº›è¿ç®—æ˜¯å¹¶è¡Œè¿›è¡Œçš„ï¼ˆçŸ©é˜µä¹˜æ³•ï¼‰
- ä¸æ˜¯784æ¬¡ä¸²è¡Œè®¡ç®—
- GPUå¯ä»¥åŒæ—¶å¤„ç†æ‰€æœ‰è¿ç®—
- ä½¿ç”¨çŸ©é˜µä¹˜æ³•ï¼š(1,784) Ã— (784,128) = (1,128) ä¸€æ¬¡å®Œæˆ

ğŸ”„ æ‰¹å¤„ç†æ¦‚å¿µï¼š
å®é™…è®­ç»ƒæ—¶ï¼Œé€šå¸¸ä¸€æ¬¡å¤„ç†å¤šå¼ å›¾ç‰‡ï¼ˆå¦‚32å¼ ï¼‰ï¼š
è¾“å…¥å½¢çŠ¶ï¼š(32, 784) - 32å¼ å›¾ç‰‡
ç¬¬1å±‚è¾“å‡ºï¼š(32, 128) - 32å¼ å›¾ç‰‡çš„128ä¸ªç‰¹å¾
ç¬¬2å±‚è¾“å‡ºï¼š(32, 64) - 32å¼ å›¾ç‰‡çš„64ä¸ªç‰¹å¾
æœ€ç»ˆè¾“å‡ºï¼š(32, 10) - 32å¼ å›¾ç‰‡çš„10ä¸ªæ¦‚ç‡
"""

model.compile(
    optimizer='adam',  # ä¼˜åŒ–å™¨
    loss='sparse_categorical_crossentropy',  # æŸå¤±å‡½æ•°
    metrics=['accuracy']  # è¯„ä¼°æŒ‡æ ‡
)
"""
1. optimizer='adam'ï¼š
   - Adamä¼˜åŒ–å™¨ï¼šè‡ªé€‚åº”å­¦ä¹ ç‡ä¼˜åŒ–ç®—æ³•
   - ä¼šè‡ªåŠ¨è°ƒæ•´å­¦ä¹ é€Ÿåº¦
   - ç±»ä¼¼äºä¸€ä¸ªæ™ºèƒ½çš„å­¦ä¹ ç­–ç•¥

2. loss='sparse_categorical_crossentropy'ï¼š
   - ç¨€ç–åˆ†ç±»äº¤å‰ç†µæŸå¤±å‡½æ•°
   - é€‚ç”¨äºå¤šåˆ†ç±»é—®é¢˜ï¼ˆ0-9å…±10ç±»ï¼‰
   - "ç¨€ç–"è¡¨ç¤ºæ ‡ç­¾æ˜¯æ•´æ•°ï¼ˆ0,1,2...ï¼‰ï¼Œä¸æ˜¯one-hotç¼–ç 
   - è¡¡é‡é¢„æµ‹æ¦‚ç‡ä¸çœŸå®æ ‡ç­¾çš„å·®è·

3. metrics=['accuracy']ï¼š
   - å‡†ç¡®ç‡ï¼šé¢„æµ‹æ­£ç¡®çš„æ ·æœ¬æ¯”ä¾‹
   - ç”¨äºç›‘æ§è®­ç»ƒè¿‡ç¨‹
   - ä¸å½±å“è®­ç»ƒï¼Œåªç”¨äºè¯„ä¼°
"""

print("æ¨¡å‹ç»“æ„:")
model.summary()

print("\n5. å¼€å§‹è®­ç»ƒæ¨¡å‹...")

history = model.fit(
    X_train_flat, y_train,      # è®­ç»ƒæ•°æ®ï¼šè¾“å…¥å’Œæ ‡ç­¾
    epochs=10,                  # è®­ç»ƒè½®æ•°ï¼šçœ‹æ•°æ®10é
    batch_size=128,             # æ‰¹æ¬¡å¤§å°ï¼šæ¯æ¬¡å¤„ç†128ä¸ªæ ·æœ¬,ä¹Ÿå°±æ˜¯128å¼ å›¾ç‰‡ï¼Œæ¯çœ‹128å¼ å›¾ç‰‡å°±é€šè¿‡æŸå¤±å‡½æ•°è®¡ç®—æ¢¯åº¦åå†æ›´æ–°æ¯ä¸ªç¥ç»å…ƒçš„æƒé‡å’Œåç½®
    validation_split=0.1,       # éªŒè¯é›†æ¯”ä¾‹ï¼š10%çš„è®­ç»ƒæ•°æ®ç”¨äºéªŒè¯
    verbose=1                   # è¾“å‡ºè¯¦ç»†ç¨‹åº¦ï¼šæ˜¾ç¤ºè®­ç»ƒè¿›åº¦
)

test_loss, test_accuracy = model.evaluate(X_test_flat, y_test, verbose=0)
"""
æ¨¡å‹è¯„ä¼°ï¼š
- model.evaluate()ï¼šåœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½
- è¿”å›æŸå¤±å€¼å’Œå‡†ç¡®ç‡
- verbose=0ï¼šä¸æ˜¾ç¤ºè¯„ä¼°è¿›åº¦
- æµ‹è¯•é›†æ˜¯æ¨¡å‹ä»æœªè§è¿‡çš„æ•°æ®ï¼Œèƒ½çœŸå®åæ˜ æ€§èƒ½
"""
print(f"æµ‹è¯•é›†å‡†ç¡®ç‡: {test_accuracy:.4f}")
print(f"æµ‹è¯•é›†æŸå¤±: {test_loss:.4f}")

"""
æ€§èƒ½æŒ‡æ ‡è§£é‡Šï¼š
- å‡†ç¡®ç‡ï¼šé¢„æµ‹æ­£ç¡®çš„æ¯”ä¾‹ï¼Œè¶Šé«˜è¶Šå¥½ï¼ˆæœ€é«˜1.0ï¼‰
- æŸå¤±å€¼ï¼šé¢„æµ‹é”™è¯¯çš„ç¨‹åº¦ï¼Œè¶Šä½è¶Šå¥½ï¼ˆæœ€ä½0.0ï¼‰
- .4fï¼šæ ¼å¼åŒ–ä¸º4ä½å°æ•°
"""
# å¯¹å‰10ä¸ªæµ‹è¯•æ ·æœ¬è¿›è¡Œé¢„æµ‹
predictions = model.predict(X_test_flat[:10], verbose=0)
"""
æ¨¡å‹é¢„æµ‹ï¼š
- model.predict()ï¼šä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹
- X_test_flat[:10]ï¼šå–å‰10ä¸ªæµ‹è¯•æ ·æœ¬
- è¿”å›æ¯ä¸ªæ ·æœ¬å¯¹åº”10ä¸ªç±»åˆ«çš„æ¦‚ç‡
- predictionså½¢çŠ¶ï¼š(10, 10): 
    [
        [0.1,0.01,0.03,....0.4] # è¿™æ˜¯ç¬¬ä¸€å¼ å›¾çš„ç‰¹å¾é›†ï¼Œè¯¥æ•°ç»„æ¯ä¸€ä¸ªå…ƒç´ å¯¹åº”çš„æ˜¯è¡¨ç¤ºå¯¹åº”åºåˆ—ç‰¹å¾çš„æ¦‚ç‡ï¼Œæ¯”å¦‚ç¬¬ä¸€ä¸ªæ˜¯0.1ï¼Œè¡¨ç¤ºè¯¥å›¾æ˜¯0çš„æ¦‚ç‡æ˜¯10%ï¼Œ
        ... # è¿™é‡Œæ€»å…±æœ‰10ä¸ªå›¾æ‰€ä»¥åƒä¸Šè¡Œçš„å†…å®¹æœ‰10è¡Œï¼Œæ„æˆå½¢çŠ¶(10,10)
    ]
"""

predicted_classes = np.argmax(predictions,axis=1)
"""
è·å–é¢„æµ‹ç±»åˆ«ï¼š
- np.argmax()ï¼šæ‰¾åˆ°æœ€å¤§å€¼çš„ç´¢å¼•
- axis=1ï¼šæ²¿ç€ç¬¬äºŒä¸ªç»´åº¦ï¼ˆç±»åˆ«ç»´åº¦ï¼‰æ‰¾æœ€å¤§å€¼
- ä¾‹å¦‚ï¼š[0.1, 0.05, 0.8, 0.02, ...] -> 2ï¼ˆç´¢å¼•2çš„å€¼æœ€å¤§ï¼‰
- predicted_classesï¼šé¢„æµ‹çš„æ•°å­—ç±»åˆ«
axis=1 å…¶å®å°±æ˜¯predictions[x],å¯¹åº”çš„å…ƒç´ ï¼Œè¿™é‡Œå°±æ˜¯å•ç‹¬ä¸€å¼ å›¾çš„é•¿åº¦ä¸º10çš„10ä¸ªç‰¹å¾çš„æ¦‚ç‡æ•°ç»„ï¼Œè·å–è¿™é‡Œæœ€å¤§çš„æ¦‚ç‡æ‰€å¯¹åº”çš„ä¸‹æ ‡
æœ€ç»ˆè¾“å‡ºçš„predicted_classesæ˜¯ä¸€ä¸ªä¸€ç»´æ•°ç»„ï¼šè¡¨ç¤ºç¬¬xå¼ å›¾å®ƒåœ¨ç‰¹å¾æ•°åˆ—ä¸­æœ€å¤§å¯èƒ½ç‰¹å¾å¯¹åº”çš„ä¸‹æ ‡å€¼
æ¯”å¦‚ï¼š
[
    [0.1, 0, 0.3, 0.05, 0, 0, 0, 0.12, 0.43 ], # è¿™æ˜¯ç¬¬ä¸€å¼ å›¾ï¼Œä»–æœ€å¤§çš„æ¦‚ç‡æ˜¯æœ€åä¸€ä¸ªä¸‹æ ‡ï¼Œå¯¹åº”çš„ç‰¹å¾æ˜¯9ï¼›åŒæ—¶ä¸‹æ ‡ä¹Ÿæ˜¯9
    ...
]
æ‰€ä»¥æœ€ç»ˆè¾“å‡ºçš„predicted_classesçš„ç¬¬ä¸€ä¸ªå…ƒç´ ä¹Ÿä¸€å®šæ˜¯9
"""

print("é¢„æµ‹ç»“æœç¤ºä¾‹:")
for i in range(5):  # æ˜¾ç¤ºå‰5ä¸ªé¢„æµ‹ç»“æœ
    true_label = y_test[i]
    predicted_label = predicted_classes[i]
    confidence = np.max(predictions[i]) * 100
    print(f"æ ·æœ¬{i+1}: çœŸå®={true_label}, é¢„æµ‹={predicted_label}, ç½®ä¿¡åº¦={confidence:.1f}%")


# ==================== å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹è¯¦è§£ ====================
print("\n8. å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹...")

plt.figure(figsize=(15, 5))  # åˆ›å»ºå¤§å›¾å½¢

# å­å›¾1ï¼šå‡†ç¡®ç‡å˜åŒ–
plt.subplot(1, 3, 1)  # 1è¡Œ3åˆ—çš„ç¬¬1ä¸ªå­å›¾
plt.plot(history.history['accuracy'], label='è®­ç»ƒå‡†ç¡®ç‡')
plt.plot(history.history['val_accuracy'], label='éªŒè¯å‡†ç¡®ç‡')
plt.title('æ¨¡å‹å‡†ç¡®ç‡')
plt.xlabel('è®­ç»ƒè½®æ•°')
plt.ylabel('å‡†ç¡®ç‡')
plt.legend()
plt.grid(True)


"""
è®­ç»ƒæ›²çº¿åˆ†æï¼š
- history.history['accuracy']ï¼šæ¯è½®çš„è®­ç»ƒå‡†ç¡®ç‡
- history.history['val_accuracy']ï¼šæ¯è½®çš„éªŒè¯å‡†ç¡®ç‡
- ç†æƒ³æƒ…å†µï¼šä¸¤æ¡çº¿éƒ½ä¸Šå‡ä¸”æ¥è¿‘
- å¦‚æœéªŒè¯å‡†ç¡®ç‡ä¸‹é™ï¼Œå¯èƒ½æ˜¯è¿‡æ‹Ÿåˆ
"""

# å­å›¾2ï¼šæŸå¤±å€¼å˜åŒ–
plt.subplot(1, 3, 2)  # ç¬¬2ä¸ªå­å›¾
plt.plot(history.history['loss'], label='è®­ç»ƒæŸå¤±')
plt.plot(history.history['val_loss'], label='éªŒè¯æŸå¤±')
plt.title('æ¨¡å‹æŸå¤±')
plt.xlabel('è®­ç»ƒè½®æ•°')
plt.ylabel('æŸå¤±å€¼')
plt.legend()
plt.grid(True)

"""
æŸå¤±æ›²çº¿åˆ†æï¼š
- ç†æƒ³æƒ…å†µï¼šä¸¤æ¡çº¿éƒ½ä¸‹é™ä¸”æ¥è¿‘
- è®­ç»ƒæŸå¤±æŒç»­ä¸‹é™ä½†éªŒè¯æŸå¤±ä¸Šå‡ï¼šè¿‡æ‹Ÿåˆ
- ä¸¤æ¡çº¿éƒ½ä¸ä¸‹é™ï¼šå­¦ä¹ ç‡å¯èƒ½å¤ªå°æˆ–æ¨¡å‹å®¹é‡ä¸è¶³
"""


# å­å›¾3ï¼šæ··æ·†çŸ©é˜µ
plt.subplot(1, 3, 3)  # ç¬¬3ä¸ªå­å›¾

# å¯¼å…¥é¢å¤–çš„åº“ç”¨äºæ··æ·†çŸ©é˜µ
from sklearn.metrics import confusion_matrix
import seaborn as sns

"""
æ–°å¯¼å…¥çš„åº“ï¼š
- sklearnï¼šæœºå™¨å­¦ä¹ å·¥å…·åº“ï¼Œç±»ä¼¼äºJSçš„å·¥å…·åº“
- seabornï¼šåŸºäºmatplotlibçš„ç»Ÿè®¡ç»˜å›¾åº“ï¼Œè®©å›¾è¡¨æ›´ç¾è§‚
"""

# å¯¹æ‰€æœ‰æµ‹è¯•æ•°æ®è¿›è¡Œé¢„æµ‹
y_pred_all = np.argmax(model.predict(X_test_flat, verbose=0), axis=1)
"""
å…¨é‡é¢„æµ‹ï¼š
- å¯¹æ‰€æœ‰10000ä¸ªæµ‹è¯•æ ·æœ¬è¿›è¡Œé¢„æµ‹
- è·å–é¢„æµ‹çš„ç±»åˆ«æ ‡ç­¾
"""

# è®¡ç®—æ··æ·†çŸ©é˜µ
cm = confusion_matrix(y_test, y_pred_all)
"""
æ··æ·†çŸ©é˜µï¼š
- æ˜¾ç¤ºæ¯ä¸ªçœŸå®ç±»åˆ«è¢«é¢„æµ‹ä¸ºå„ä¸ªç±»åˆ«çš„æ•°é‡
- å¯¹è§’çº¿ï¼šé¢„æµ‹æ­£ç¡®çš„æ•°é‡
- éå¯¹è§’çº¿ï¼šé¢„æµ‹é”™è¯¯çš„æ•°é‡
- 10x10çš„çŸ©é˜µï¼Œå¯¹åº”0-9è¿™10ä¸ªæ•°å­—
"""

# ç»˜åˆ¶çƒ­åŠ›å›¾
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('æ··æ·†çŸ©é˜µ')
plt.xlabel('é¢„æµ‹æ ‡ç­¾')
plt.ylabel('çœŸå®æ ‡ç­¾')


"""
çƒ­åŠ›å›¾å‚æ•°ï¼š
- annot=Trueï¼šåœ¨æ¯ä¸ªæ ¼å­ä¸­æ˜¾ç¤ºæ•°å€¼
- fmt='d'ï¼šæ•°å€¼æ ¼å¼ä¸ºæ•´æ•°
- cmap='Blues'ï¼šä½¿ç”¨è“è‰²è‰²å½©æ˜ å°„
"""

plt.tight_layout()  # è°ƒæ•´å¸ƒå±€
plt.show()


# ==================== é¢„æµ‹ç»“æœå±•ç¤ºè¯¦è§£ ====================
print("\n9. å±•ç¤ºé¢„æµ‹ç»“æœ...")

plt.figure(figsize=(12, 6))
for i in range(10):  # æ˜¾ç¤ºå‰10ä¸ªé¢„æµ‹ç»“æœ
    plt.subplot(2, 5, i+1)
    plt.imshow(X_test[i], cmap='gray')  # æ˜¾ç¤ºåŸå§‹å›¾åƒ

    # è®¡ç®—ç½®ä¿¡åº¦
    confidence = np.max(predictions[i]) * 100

    # è®¾ç½®æ ‡é¢˜ï¼Œæ˜¾ç¤ºçœŸå®å€¼ã€é¢„æµ‹å€¼å’Œç½®ä¿¡åº¦
    plt.title(f'çœŸå®: {y_test[i]}, é¢„æµ‹: {predicted_classes[i]}\nç½®ä¿¡åº¦: {confidence:.1f}%')
    plt.axis('off')

plt.suptitle('é¢„æµ‹ç»“æœç¤ºä¾‹')
plt.tight_layout()
plt.show()

# ==================== æ¨¡å‹ä¿å­˜è¯¦è§£ ====================
print("\n10. ä¿å­˜æ¨¡å‹...")

model.save('mnist_model.h5')