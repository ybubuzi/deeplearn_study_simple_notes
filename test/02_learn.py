import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

# 设置matplotlib中文字体显示
plt.rcParams['font.sans-serif'] = ['SimHei']  # 设置字体为黑体
plt.rcParams['axes.unicode_minus'] = False  # 正常显示负号

# 加载MNIST数据集
# 该代码是调用官方库，下载官方的测试训练集，存放至本地%UserData%/.keras/datasets下
# 再具体点就是该目录下的mnist.npz文件，可以更改后缀后使用zip打开查看
(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()
"""
该压缩包解压后会有4个文件，y_train.npy、x_train.npy、y_test.npy、x_test.npy
训练集图形是一个灰度图集，因此像素28*28，每一个像素点仅由一个灰度值构成
一般的彩色图像的构成由三个数值或四个构成，及R、G、B、A
看名字得知就是训练数据集和验证数据集
- X_train：训练图像数据，形状为(60000, 28, 28)
    x的形状为[
                [
                    [1,2,3,4,5,6,7,....,28] # 第1行
                    ...
                    [1,2,3,4,5,6,7,....,28] # 第28行
                ]，
                ... 这里总共有60000个
            ]
- y_train：训练标签数据，形状为(60000,)，值为0-9
    y的形状为: [1,2,3,4...0] 这里一共有60000长度

- X_test：测试图像数据，形状为(10000, 28, 28)  
- y_test：测试标签数据，形状为(10000,)
"""
# 这里获取了整个矩阵空间中最明亮的像素点和最暗的像素点，注意该图是灰度图，仅有灰度（亮度）信息
print(f"像素值范围: {X_train.min()} - {X_train.max()}")

"""
min和max方法获取整个矩阵空间中的极值
"""

# 数据预处理
# 数据归一化：将像素值从0-255缩放到0-1
X_train = X_train.astype('float32') / 255.0
X_test = X_test.astype('float32') / 255.0

"""
将数据等比压缩至0~1；
由于一个像素信息最大为255，这里只需除以255即可缩放至1
"""

X_train_flat = X_train.reshape(X_train.shape[0], -1)
X_test_flat = X_test.reshape(X_test.shape[0], -1)

"""
将张量扁平化
X_train.shape本身是一个元组(60000,28,28),
所以这里的X_train.shape[0]，其实就是60000，
这里的意思是保留60000个元素，而每个元素内的子元素数量由np自动计算，这里就是28*28来填充了
最终就变成了
x: [
        [255,255,255,255,255,......,255] # 第一张图的信息，总共28*28 = 784个元素
        ...
        ... 
        [255,255,255,255,255,......,255] # 这是第60000张图的信息，也是784个元素
    ]
"""

plt.figure(figsize=(12, 4))

for i in range(10):
    plt.subplot(2, 5, i + 1)
    plt.imshow(X_train[i], cmap='gray')
    plt.title(f'标签: {y_train[i]}')  # 设置子图标题
    plt.axis('off')  # 隐藏坐标轴
plt.suptitle('MNIST数据集样本')  # 设置整个图形的标题
plt.tight_layout()  # 自动调整子图间距
plt.show()  # 显示图形
"""
这里创建了一个2行5列的窗口，每一个单元格显示一张图片
"""

model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu', input_shape=(784,), name='hidden_layer1'),
    tf.keras.layers.Dropout(0.2, name='dropout_layer'),
    tf.keras.layers.Dense(64, activation='relu', name='hidden_layer2'),
    tf.keras.layers.Dense(10, activation='softmax', name='output_layer')  # 添加输出层
])
"""
第一个隐藏层：
这里的128表示有128个神经元，每一个神经元对一个图片都会有唯一的特征输出
最终输出128个特征
activation='relu'：是激活函数，对神经元的输出值最小为0.伪代码为 max(0,x)
input=(784,)表示上一层的输入形状，因为在此之前我们对每一张图片进行了扁平化，一张图片就是由784个数字构成的一维数值

这里的128个神经元都会输入完整的784个像素点
输入层(784个像素)    隐藏层1(128个神经元)
     ↓                    ↓
   像素1  ────────────→ 神经元1
   像素2  ────────────→ 神经元1  
   像素3  ────────────→ 神经元1
   ...    ────────────→ 神经元1
   像素784 ───────────→ 神经元1

   像素1  ────────────→ 神经元2
   像素2  ────────────→ 神经元2
   ...    ────────────→ 神经元2
   像素784 ───────────→ 神经元2

   ... (每个神经元都连接所有784个像素)

Dropout层用于随机关闭某些神经元，就是让输出特征减少；防止模型依赖某一固定的特征而产生过拟合；
该层只会在训练时生效
过拟合问题：
    - 模型在训练数据上表现很好，但在新数据上表现差
    - 类似于死记硬背，不能举一反三
    - Dropout通过增加随机性来缓解这个问题
也就是说经过该层的128个特征点，会随机丢失20%；
第二个隐藏层：
这一层只有64个神经元，也就是会输出64个特征点；
这里的输入是上一个隐藏层的输出，也就是上一层输出的128个特征点，经过dropout后在与该层神经点进行链接；
这里每一个神经元都会吧128个特征“看”一遍，并

最后的是输出层
输出层有10个表示有10个状态，正好对应0~9的数字
activation='softmax'：
       - Softmax激活函数，将输出转换为概率分布
       - 所有输出概率加起来等于1
       - 类似于投票结果的百分比
例如输出：[0.1, 0.05, 0.8, 0.02, 0.01, 0.01, 0.005, 0.005, 0.005, 0.005]
    表示：数字2的概率是80%，数字0的概率是10%，等等，这里的2是指该输出层的第三个神经元，也就是下标为2的值
"""

# ============================================================================
# 🧠 神经网络详细解析
# ============================================================================

"""
📊 重要理解纠正：数据流动过程
❌ 错误理解：784个像素依次传递，一张图片计算784次
✅ 正确理解：所有784个像素同时并行传递，一张图片只计算一次！

🔄 一张图片的完整数据流动：
输入: [784个像素] → 隐藏层1: [128个特征] → Dropout → 隐藏层2: [64个特征] → 输出: [10个概率]
形状: (1,784)     →        (1,128)        →        →       (1,64)       →      (1,10)

🧮 神经元计算公式详解（关键理解）：

对于第一层的神经元i（i=1到128）：
神经元i的输出 = ReLU(p1×wi1 + p2×wi2 + p3×wi3 + ... + p784×wi784 + bi)

其中：
- p1, p2, ..., p784：784个像素值（0-1之间的小数）
- wi1, wi2, ..., wi784：神经元i对应的784个权重（每个像素都有不同的权重！）
- bi：神经元i的偏置
- ReLU：激活函数 max(0,x)

❌ 我之前的错误理解：p1×w + p2×w + p3×w + ... + p784×w + b（所有像素用相同权重）
✅ 正确理解：p1×w1 + p2×w2 + p3×w3 + ... + p784×w784 + b（每个像素有不同权重）

🎯 为什么每个像素需要不同权重？
- 权重决定了该像素对神经元输出的重要性
- 正权重：该像素亮时增加输出
- 负权重：该像素亮时减少输出
- 零权重：该像素被忽略
- 不同神经元通过不同的权重组合来检测不同的特征

💾 权重矩阵结构详解：
第一层权重矩阵形状：(784, 128)
        神经元1  神经元2  神经元3  ...  神经元128
像素1   [  w1,1    w1,2    w1,3   ...   w1,128  ]
像素2   [  w2,1    w2,2    w2,3   ...   w2,128  ]
像素3   [  w3,1    w3,2    w3,3   ...   w3,128  ]
...     [   ...     ...     ...   ...     ...   ]
像素784 [ w784,1  w784,2  w784,3  ...  w784,128 ]

总参数：784×128 + 128 = 100,480个参数（权重+偏置）

🎯 神经元的作用（特征检测器）：
- 神经元1：可能专门检测水平边缘特征
- 神经元2：可能专门检测垂直边缘特征
- 神经元3：可能专门检测圆形特征
- 神经元4：可能专门检测角点特征
- ...
- 神经元128：可能专门检测其他特定特征
- 每个神经元通过不同的权重组合来"关注"图像的不同方面

🔍 第二层神经元计算：
对于第二层的神经元j（j=1到64）：
神经元j的输出 = ReLU(f1×wj1 + f2×wj2 + ... + f128×wj128 + bj)

其中：
- f1, f2, ..., f128：来自第一层的128个特征值
- wj1, wj2, ..., wj128：神经元j对应的128个权重
- bj：神经元j的偏置

🎯 输出层计算：
对于输出层的神经元k（k=1到10，对应数字0-9）：
神经元k的输出 = Softmax(g1×wk1 + g2×wk2 + ... + g64×wk64 + bk)

其中：
- g1, g2, ..., g64：来自第二层的64个特征值
- Softmax：将输出转换为概率分布，所有输出和为1

📈 损失计算详解：
1. 模型输出：10个概率值，如 [0.1, 0.05, 0.8, 0.02, 0.01, 0.01, 0.005, 0.005, 0.005, 0.005]
2. 真实标签：如 2（表示这张图片是数字2）
3. 交叉熵损失：损失 = -log(预测概率[真实类别]) = -log(0.8) = 0.223
4. 批次损失：所有样本损失的平均值

⚡ 计算复杂度分析：
单张图片的计算次数：
• 第1层：784 × 128 = 100,352 次乘法
• 第2层：128 × 64 = 8,192 次乘法
• 输出层：64 × 10 = 640 次乘法
• 总计：约 109,184 次乘法运算

💡 关键理解：
- 这些运算是并行进行的（矩阵乘法）
- 不是784次串行计算
- GPU可以同时处理所有运算
- 使用矩阵乘法：(1,784) × (784,128) = (1,128) 一次完成

🔄 批处理概念：
实际训练时，通常一次处理多张图片（如32张）：
输入形状：(32, 784) - 32张图片
第1层输出：(32, 128) - 32张图片的128个特征
第2层输出：(32, 64) - 32张图片的64个特征
最终输出：(32, 10) - 32张图片的10个概率
"""

model.compile(
    optimizer='adam',  # 优化器
    loss='sparse_categorical_crossentropy',  # 损失函数
    metrics=['accuracy']  # 评估指标
)
"""
1. optimizer='adam'：
   - Adam优化器：自适应学习率优化算法
   - 会自动调整学习速度
   - 类似于一个智能的学习策略

2. loss='sparse_categorical_crossentropy'：
   - 稀疏分类交叉熵损失函数
   - 适用于多分类问题（0-9共10类）
   - "稀疏"表示标签是整数（0,1,2...），不是one-hot编码
   - 衡量预测概率与真实标签的差距

3. metrics=['accuracy']：
   - 准确率：预测正确的样本比例
   - 用于监控训练过程
   - 不影响训练，只用于评估
"""

print("模型结构:")
model.summary()

print("\n5. 开始训练模型...")

history = model.fit(
    X_train_flat, y_train,      # 训练数据：输入和标签
    epochs=10,                  # 训练轮数：看数据10遍
    batch_size=128,             # 批次大小：每次处理128个样本,也就是128张图片，每看128张图片就通过损失函数计算梯度后再更新每个神经元的权重和偏置
    validation_split=0.1,       # 验证集比例：10%的训练数据用于验证
    verbose=1                   # 输出详细程度：显示训练进度
)

test_loss, test_accuracy = model.evaluate(X_test_flat, y_test, verbose=0)
"""
模型评估：
- model.evaluate()：在测试集上评估模型性能
- 返回损失值和准确率
- verbose=0：不显示评估进度
- 测试集是模型从未见过的数据，能真实反映性能
"""
print(f"测试集准确率: {test_accuracy:.4f}")
print(f"测试集损失: {test_loss:.4f}")

"""
性能指标解释：
- 准确率：预测正确的比例，越高越好（最高1.0）
- 损失值：预测错误的程度，越低越好（最低0.0）
- .4f：格式化为4位小数
"""
# 对前10个测试样本进行预测
predictions = model.predict(X_test_flat[:10], verbose=0)
"""
模型预测：
- model.predict()：使用训练好的模型进行预测
- X_test_flat[:10]：取前10个测试样本
- 返回每个样本对应10个类别的概率
- predictions形状：(10, 10): 
    [
        [0.1,0.01,0.03,....0.4] # 这是第一张图的特征集，该数组每一个元素对应的是表示对应序列特征的概率，比如第一个是0.1，表示该图是0的概率是10%，
        ... # 这里总共有10个图所以像上行的内容有10行，构成形状(10,10)
    ]
"""

predicted_classes = np.argmax(predictions,axis=1)
"""
获取预测类别：
- np.argmax()：找到最大值的索引
- axis=1：沿着第二个维度（类别维度）找最大值
- 例如：[0.1, 0.05, 0.8, 0.02, ...] -> 2（索引2的值最大）
- predicted_classes：预测的数字类别
axis=1 其实就是predictions[x],对应的元素，这里就是单独一张图的长度为10的10个特征的概率数组，获取这里最大的概率所对应的下标
最终输出的predicted_classes是一个一维数组：表示第x张图它在特征数列中最大可能特征对应的下标值
比如：
[
    [0.1, 0, 0.3, 0.05, 0, 0, 0, 0.12, 0.43 ], # 这是第一张图，他最大的概率是最后一个下标，对应的特征是9；同时下标也是9
    ...
]
所以最终输出的predicted_classes的第一个元素也一定是9
"""

print("预测结果示例:")
for i in range(5):  # 显示前5个预测结果
    true_label = y_test[i]
    predicted_label = predicted_classes[i]
    confidence = np.max(predictions[i]) * 100
    print(f"样本{i+1}: 真实={true_label}, 预测={predicted_label}, 置信度={confidence:.1f}%")


# ==================== 可视化训练过程详解 ====================
print("\n8. 可视化训练过程...")

plt.figure(figsize=(15, 5))  # 创建大图形

# 子图1：准确率变化
plt.subplot(1, 3, 1)  # 1行3列的第1个子图
plt.plot(history.history['accuracy'], label='训练准确率')
plt.plot(history.history['val_accuracy'], label='验证准确率')
plt.title('模型准确率')
plt.xlabel('训练轮数')
plt.ylabel('准确率')
plt.legend()
plt.grid(True)


"""
训练曲线分析：
- history.history['accuracy']：每轮的训练准确率
- history.history['val_accuracy']：每轮的验证准确率
- 理想情况：两条线都上升且接近
- 如果验证准确率下降，可能是过拟合
"""

# 子图2：损失值变化
plt.subplot(1, 3, 2)  # 第2个子图
plt.plot(history.history['loss'], label='训练损失')
plt.plot(history.history['val_loss'], label='验证损失')
plt.title('模型损失')
plt.xlabel('训练轮数')
plt.ylabel('损失值')
plt.legend()
plt.grid(True)

"""
损失曲线分析：
- 理想情况：两条线都下降且接近
- 训练损失持续下降但验证损失上升：过拟合
- 两条线都不下降：学习率可能太小或模型容量不足
"""


# 子图3：混淆矩阵
plt.subplot(1, 3, 3)  # 第3个子图

# 导入额外的库用于混淆矩阵
from sklearn.metrics import confusion_matrix
import seaborn as sns

"""
新导入的库：
- sklearn：机器学习工具库，类似于JS的工具库
- seaborn：基于matplotlib的统计绘图库，让图表更美观
"""

# 对所有测试数据进行预测
y_pred_all = np.argmax(model.predict(X_test_flat, verbose=0), axis=1)
"""
全量预测：
- 对所有10000个测试样本进行预测
- 获取预测的类别标签
"""

# 计算混淆矩阵
cm = confusion_matrix(y_test, y_pred_all)
"""
混淆矩阵：
- 显示每个真实类别被预测为各个类别的数量
- 对角线：预测正确的数量
- 非对角线：预测错误的数量
- 10x10的矩阵，对应0-9这10个数字
"""

# 绘制热力图
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('混淆矩阵')
plt.xlabel('预测标签')
plt.ylabel('真实标签')


"""
热力图参数：
- annot=True：在每个格子中显示数值
- fmt='d'：数值格式为整数
- cmap='Blues'：使用蓝色色彩映射
"""

plt.tight_layout()  # 调整布局
plt.show()


# ==================== 预测结果展示详解 ====================
print("\n9. 展示预测结果...")

plt.figure(figsize=(12, 6))
for i in range(10):  # 显示前10个预测结果
    plt.subplot(2, 5, i+1)
    plt.imshow(X_test[i], cmap='gray')  # 显示原始图像

    # 计算置信度
    confidence = np.max(predictions[i]) * 100

    # 设置标题，显示真实值、预测值和置信度
    plt.title(f'真实: {y_test[i]}, 预测: {predicted_classes[i]}\n置信度: {confidence:.1f}%')
    plt.axis('off')

plt.suptitle('预测结果示例')
plt.tight_layout()
plt.show()

# ==================== 模型保存详解 ====================
print("\n10. 保存模型...")

model.save('mnist_model.h5')