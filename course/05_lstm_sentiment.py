"""
TensorFlow æ·±åº¦å­¦ä¹ å…¥é—¨ç¤ºä¾‹ 5: LSTMæ–‡æœ¬æƒ…æ„Ÿåˆ†æ
å­¦ä¹ LSTMå¤„ç†åºåˆ—æ•°æ®çš„å¼ºå¤§èƒ½åŠ›
åˆ†æç”µå½±è¯„è®ºçš„æƒ…æ„Ÿï¼ˆæ­£é¢/è´Ÿé¢ï¼‰
"""

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

# è®¾ç½®ä¸­æ–‡å­—ä½“æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

print("TensorFlowç‰ˆæœ¬:", tf.__version__)
print("=" * 60)
print("ğŸ­ LSTMæ–‡æœ¬æƒ…æ„Ÿåˆ†æå…¥é—¨")
print("=" * 60)

# 1. åŠ è½½IMDBç”µå½±è¯„è®ºæ•°æ®é›†
print("\n1. åŠ è½½IMDBç”µå½±è¯„è®ºæ•°æ®...")

# åªä½¿ç”¨æœ€å¸¸è§çš„10000ä¸ªå•è¯
max_features = 10000
# æ¯ä¸ªè¯„è®ºæœ€å¤šä½¿ç”¨500ä¸ªå•è¯
max_length = 500

print("æ­£åœ¨ä¸‹è½½IMDBæ•°æ®é›†...")
(X_train, y_train), (X_test, y_test) = tf.keras.datasets.imdb.load_data(
    num_words=max_features
)

print(f"è®­ç»ƒæ ·æœ¬æ•°: {len(X_train)}")
print(f"æµ‹è¯•æ ·æœ¬æ•°: {len(X_test)}")
print(f"æ ‡ç­¾: 0=è´Ÿé¢è¯„è®º, 1=æ­£é¢è¯„è®º")

# æŸ¥çœ‹æ•°æ®ç»“æ„
print(f"\næ•°æ®ç¤ºä¾‹:")
print(f"ç¬¬ä¸€ä¸ªè¯„è®ºé•¿åº¦: {len(X_train[0])} ä¸ªå•è¯")
print(f"ç¬¬ä¸€ä¸ªè¯„è®ºæ ‡ç­¾: {y_train[0]} ({'æ­£é¢' if y_train[0] == 1 else 'è´Ÿé¢'})")
print(f"è¯„è®ºå†…å®¹ï¼ˆæ•°å­—ç¼–ç ï¼‰: {X_train[0][:20]}...")

# 2. æ•°æ®é¢„å¤„ç†
print("\n2. æ•°æ®é¢„å¤„ç†...")

# å°†åºåˆ—å¡«å……åˆ°ç›¸åŒé•¿åº¦
# çŸ­çš„åºåˆ—ç”¨0å¡«å……ï¼Œé•¿çš„åºåˆ—æˆªæ–­
X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, maxlen=max_length)
X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen=max_length)

print(f"å¡«å……åçš„æ•°æ®å½¢çŠ¶:")
print(f"X_train: {X_train.shape}")  # (æ ·æœ¬æ•°, åºåˆ—é•¿åº¦)
print(f"X_test: {X_test.shape}")
print(f"y_train: {y_train.shape}")

# æŸ¥çœ‹å¡«å……åçš„æ•°æ®
print(f"\nå¡«å……åçš„ç¬¬ä¸€ä¸ªè¯„è®º:")
print(f"å‰20ä¸ªè¯: {X_train[0][:20]}")
print(f"å20ä¸ªè¯: {X_train[0][-20:]}")

# 3. æ„å»ºLSTMæ¨¡å‹
print("\n3. æ„å»ºLSTMæ¨¡å‹...")

model = tf.keras.Sequential([
    # è¯åµŒå…¥å±‚ï¼šå°†å•è¯IDè½¬æ¢ä¸ºå¯†é›†å‘é‡
    # ç±»ä¼¼äºç»™æ¯ä¸ªå•è¯ä¸€ä¸ª"èº«ä»½è¯"å‘é‡
    tf.keras.layers.Embedding(
        input_dim=max_features,  # è¯æ±‡è¡¨å¤§å°
        output_dim=128,          # æ¯ä¸ªè¯çš„å‘é‡ç»´åº¦
        input_length=max_length, # è¾“å…¥åºåˆ—é•¿åº¦
        name='embedding'
    ),
    
    # LSTMå±‚ï¼šå­¦ä¹ åºåˆ—ä¸­çš„é•¿æœŸä¾èµ–å…³ç³»
    tf.keras.layers.LSTM(64, dropout=0.5, name='lstm'),
    
    # è¾“å‡ºå±‚ï¼šäºŒåˆ†ç±»ï¼ˆæ­£é¢/è´Ÿé¢ï¼‰
    tf.keras.layers.Dense(1, activation='sigmoid', name='output')
])

# ç¼–è¯‘æ¨¡å‹
model.compile(
    optimizer='adam',
    loss='binary_crossentropy',  # äºŒåˆ†ç±»äº¤å‰ç†µ
    metrics=['accuracy']
)

print("LSTMæ¨¡å‹ç»“æ„:")
model.summary()

# 4. è®­ç»ƒæ¨¡å‹
print("\n4. å¼€å§‹è®­ç»ƒLSTM...")

history = model.fit(
    X_train, y_train,
    epochs=5,  # è®­ç»ƒ5è½®ï¼ˆLSTMè®­ç»ƒè¾ƒæ…¢ï¼‰
    batch_size=32,
    validation_data=(X_test, y_test),
    verbose=1
)

# 5. è¯„ä¼°æ¨¡å‹
print("\n5. æ¨¡å‹è¯„ä¼°...")
test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)
print(f"æµ‹è¯•å‡†ç¡®ç‡: {test_accuracy:.4f}")
print(f"æµ‹è¯•æŸå¤±: {test_loss:.4f}")

# 6. è¿›è¡Œé¢„æµ‹
print("\n6. è¿›è¡Œé¢„æµ‹...")
predictions = model.predict(X_test[:10], verbose=0)

print("é¢„æµ‹ç»“æœç¤ºä¾‹:")
for i in range(10):
    prob = predictions[i][0]
    predicted = "æ­£é¢" if prob > 0.5 else "è´Ÿé¢"
    actual = "æ­£é¢" if y_test[i] == 1 else "è´Ÿé¢"
    print(f"æ ·æœ¬{i+1}: é¢„æµ‹={predicted}({prob:.3f}), å®é™…={actual}")

# 7. å¯è§†åŒ–ç»“æœ
plt.figure(figsize=(12, 8))

# è®­ç»ƒè¿‡ç¨‹
plt.subplot(2, 2, 1)
plt.plot(history.history['accuracy'], label='è®­ç»ƒå‡†ç¡®ç‡')
plt.plot(history.history['val_accuracy'], label='éªŒè¯å‡†ç¡®ç‡')
plt.title('LSTMæ¨¡å‹å‡†ç¡®ç‡')
plt.xlabel('è®­ç»ƒè½®æ•°')
plt.ylabel('å‡†ç¡®ç‡')
plt.legend()
plt.grid(True)

plt.subplot(2, 2, 2)
plt.plot(history.history['loss'], label='è®­ç»ƒæŸå¤±')
plt.plot(history.history['val_loss'], label='éªŒè¯æŸå¤±')
plt.title('LSTMæ¨¡å‹æŸå¤±')
plt.xlabel('è®­ç»ƒè½®æ•°')
plt.ylabel('æŸå¤±å€¼')
plt.legend()
plt.grid(True)

# é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ
plt.subplot(2, 2, 3)
all_predictions = model.predict(X_test, verbose=0)
plt.hist(all_predictions, bins=50, alpha=0.7)
plt.title('é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ')
plt.xlabel('é¢„æµ‹æ¦‚ç‡')
plt.ylabel('é¢‘æ¬¡')
plt.axvline(x=0.5, color='red', linestyle='--', label='åˆ†ç±»é˜ˆå€¼')
plt.legend()
plt.grid(True)

# æ··æ·†çŸ©é˜µ
plt.subplot(2, 2, 4)
from sklearn.metrics import confusion_matrix
import seaborn as sns

y_pred = (all_predictions > 0.5).astype(int)
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('æ··æ·†çŸ©é˜µ')
plt.xlabel('é¢„æµ‹æ ‡ç­¾')
plt.ylabel('çœŸå®æ ‡ç­¾')

plt.tight_layout()
plt.show()

# 8. æ–‡æœ¬è§£ç ç¤ºä¾‹ï¼ˆå¯é€‰ï¼‰
print("\n7. æ–‡æœ¬è§£ç ç¤ºä¾‹...")

# è·å–å•è¯ç´¢å¼•å­—å…¸
word_index = tf.keras.datasets.imdb.get_word_index()
# åˆ›å»ºåå‘å­—å…¸ï¼ˆä»ç´¢å¼•åˆ°å•è¯ï¼‰
reverse_word_index = {value: key for key, value in word_index.items()}

def decode_review(encoded_review):
    """å°†æ•°å­—ç¼–ç çš„è¯„è®ºè½¬æ¢å›æ–‡æœ¬"""
    # æ³¨æ„ï¼šç´¢å¼•åç§»äº†3ä½ï¼Œå› ä¸º0,1,2æ˜¯ä¿ç•™çš„ç‰¹æ®Šæ ‡è®°
    return ' '.join([reverse_word_index.get(i - 3, '?') for i in encoded_review if i > 0])

# æ˜¾ç¤ºä¸€ä¸ªåŸå§‹è¯„è®º
print("åŸå§‹è¯„è®ºç¤ºä¾‹:")
sample_review = decode_review(X_test[0])
print(f"è¯„è®ºå†…å®¹: {sample_review[:200]}...")
print(f"å®é™…æ ‡ç­¾: {'æ­£é¢' if y_test[0] == 1 else 'è´Ÿé¢'}")
print(f"é¢„æµ‹æ¦‚ç‡: {all_predictions[0][0]:.3f}")
print(f"é¢„æµ‹æ ‡ç­¾: {'æ­£é¢' if all_predictions[0][0] > 0.5 else 'è´Ÿé¢'}")

# 9. ä¿å­˜æ¨¡å‹
model.save('lstm_sentiment_model.h5')
print("\næ¨¡å‹å·²ä¿å­˜ä¸º 'lstm_sentiment_model.h5'")

print("\n" + "=" * 60)
print("âœ… LSTMæ–‡æœ¬æƒ…æ„Ÿåˆ†æå®Œæˆï¼")
print("=" * 60)
print("ğŸ“š å­¦åˆ°çš„æ¦‚å¿µ:")
print("1. è¯åµŒå…¥(Embedding)ï¼šå°†å•è¯è½¬æ¢ä¸ºå‘é‡")
print("2. LSTMï¼šå¤„ç†é•¿åºåˆ—çš„è®°å¿†ç½‘ç»œ")
print("3. åºåˆ—å¡«å……ï¼šç»Ÿä¸€è¾“å…¥é•¿åº¦")
print("4. äºŒåˆ†ç±»ï¼šsigmoidæ¿€æ´»å‡½æ•°")
print("5. æ–‡æœ¬é¢„å¤„ç†ï¼šç¼–ç å’Œè§£ç ")
print("\nğŸ” LSTM vs ç®€å•RNNçš„ä¼˜åŠ¿:")
print("- èƒ½è®°ä½æ›´é•¿çš„å†å²ä¿¡æ¯")
print("- è§£å†³äº†æ¢¯åº¦æ¶ˆå¤±é—®é¢˜")
print("- æ›´é€‚åˆå¤„ç†é•¿æ–‡æœ¬åºåˆ—")
print("=" * 60)
