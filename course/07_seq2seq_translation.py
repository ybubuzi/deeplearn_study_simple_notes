"""
TensorFlow æ·±åº¦å­¦ä¹ å…¥é—¨ç¤ºä¾‹ 7: åºåˆ—åˆ°åºåˆ—æ¨¡å‹ (Seq2Seq)
å­¦ä¹ ç¼–ç å™¨-è§£ç å™¨æ¶æ„
ç®€å•çš„æ•°å­—åºåˆ—ç¿»è¯‘ï¼ˆä¾‹å¦‚ï¼šå°†æ•°å­—åºåˆ—åè½¬ï¼‰
"""

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

# è®¾ç½®ä¸­æ–‡å­—ä½“æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

print("TensorFlowç‰ˆæœ¬:", tf.__version__)
print("=" * 60)
print("ğŸ”„ åºåˆ—åˆ°åºåˆ—æ¨¡å‹å…¥é—¨")
print("=" * 60)

# 1. ç”Ÿæˆåºåˆ—ç¿»è¯‘æ•°æ®
print("\n1. ç”Ÿæˆåºåˆ—ç¿»è¯‘æ•°æ®...")

def create_sequence_data(num_samples=10000, seq_length=10):
    """
    åˆ›å»ºåºåˆ—ç¿»è¯‘æ•°æ®
    ä»»åŠ¡ï¼šå°†è¾“å…¥åºåˆ—åè½¬
    ä¾‹å¦‚ï¼š[1,2,3,4,5] -> [5,4,3,2,1]
    """
    np.random.seed(42)
    
    # è¯æ±‡è¡¨å¤§å°ï¼ˆæ•°å­—0-9ï¼‰
    vocab_size = 10
    
    input_sequences = []
    target_sequences = []
    
    for _ in range(num_samples):
        # ç”Ÿæˆéšæœºåºåˆ—
        seq = np.random.randint(1, vocab_size, seq_length)
        
        # è¾“å…¥åºåˆ—
        input_sequences.append(seq)
        
        # ç›®æ ‡åºåˆ—ï¼ˆåè½¬ï¼‰
        target_sequences.append(seq[::-1])
    
    return np.array(input_sequences), np.array(target_sequences), vocab_size

# ç”Ÿæˆæ•°æ®
seq_length = 8
input_seqs, target_seqs, vocab_size = create_sequence_data(10000, seq_length)

print(f"ç”Ÿæˆäº† {len(input_seqs)} ä¸ªåºåˆ—å¯¹")
print(f"åºåˆ—é•¿åº¦: {seq_length}")
print(f"è¯æ±‡è¡¨å¤§å°: {vocab_size}")
print(f"è¾“å…¥å½¢çŠ¶: {input_seqs.shape}")
print(f"ç›®æ ‡å½¢çŠ¶: {target_seqs.shape}")

# æ˜¾ç¤ºæ•°æ®ç¤ºä¾‹
print(f"\næ•°æ®ç¤ºä¾‹:")
for i in range(5):
    print(f"è¾“å…¥: {input_seqs[i]} -> ç›®æ ‡: {target_seqs[i]}")

# 2. æ•°æ®é¢„å¤„ç†
print("\n2. æ•°æ®é¢„å¤„ç†...")

# åˆ†å‰²è®­ç»ƒå’Œæµ‹è¯•é›†
train_size = int(0.8 * len(input_seqs))
X_train, X_test = input_seqs[:train_size], input_seqs[train_size:]
y_train, y_test = target_seqs[:train_size], target_seqs[train_size:]

print(f"è®­ç»ƒé›†å¤§å°: {len(X_train)}")
print(f"æµ‹è¯•é›†å¤§å°: {len(X_test)}")

# ä¸ºç›®æ ‡åºåˆ—æ·»åŠ å¼€å§‹å’Œç»“æŸæ ‡è®°
# 0: å¡«å……ç¬¦, 10: å¼€å§‹æ ‡è®°, 11: ç»“æŸæ ‡è®°
START_TOKEN = vocab_size
END_TOKEN = vocab_size + 1
vocab_size_extended = vocab_size + 2

# åˆ›å»ºè§£ç å™¨è¾“å…¥å’Œç›®æ ‡
def prepare_decoder_data(target_sequences):
    """
    å‡†å¤‡è§£ç å™¨çš„è¾“å…¥å’Œç›®æ ‡æ•°æ®
    è§£ç å™¨è¾“å…¥: [START, ç›®æ ‡åºåˆ—]
    è§£ç å™¨ç›®æ ‡: [ç›®æ ‡åºåˆ—, END]
    """
    decoder_input = np.zeros((len(target_sequences), seq_length + 1))
    decoder_target = np.zeros((len(target_sequences), seq_length + 1))
    
    for i, seq in enumerate(target_sequences):
        # è§£ç å™¨è¾“å…¥ï¼šå¼€å§‹æ ‡è®° + ç›®æ ‡åºåˆ—
        decoder_input[i, 0] = START_TOKEN
        decoder_input[i, 1:seq_length+1] = seq
        
        # è§£ç å™¨ç›®æ ‡ï¼šç›®æ ‡åºåˆ— + ç»“æŸæ ‡è®°
        decoder_target[i, :seq_length] = seq
        decoder_target[i, seq_length] = END_TOKEN
    
    return decoder_input, decoder_target

decoder_input_train, decoder_target_train = prepare_decoder_data(y_train)
decoder_input_test, decoder_target_test = prepare_decoder_data(y_test)

print(f"è§£ç å™¨è¾“å…¥å½¢çŠ¶: {decoder_input_train.shape}")
print(f"è§£ç å™¨ç›®æ ‡å½¢çŠ¶: {decoder_target_train.shape}")

# 3. æ„å»ºSeq2Seqæ¨¡å‹
print("\n3. æ„å»ºSeq2Seqæ¨¡å‹...")

# æ¨¡å‹å‚æ•°
embedding_dim = 64
hidden_units = 128

# ç¼–ç å™¨
encoder_inputs = tf.keras.Input(shape=(seq_length,), name='encoder_inputs')
encoder_embedding = tf.keras.layers.Embedding(vocab_size_extended, embedding_dim)(encoder_inputs)
encoder_lstm = tf.keras.layers.LSTM(hidden_units, return_state=True, name='encoder_lstm')
encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)
encoder_states = [state_h, state_c]

# è§£ç å™¨
decoder_inputs = tf.keras.Input(shape=(seq_length + 1,), name='decoder_inputs')
decoder_embedding = tf.keras.layers.Embedding(vocab_size_extended, embedding_dim)(decoder_inputs)
decoder_lstm = tf.keras.layers.LSTM(hidden_units, return_sequences=True, return_state=True, name='decoder_lstm')
decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)
decoder_dense = tf.keras.layers.Dense(vocab_size_extended, activation='softmax', name='decoder_dense')
decoder_outputs = decoder_dense(decoder_outputs)

# å®Œæ•´æ¨¡å‹
model = tf.keras.Model([encoder_inputs, decoder_inputs], decoder_outputs, name='seq2seq_model')

# ç¼–è¯‘æ¨¡å‹
model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

print("Seq2Seqæ¨¡å‹ç»“æ„:")
model.summary()

# 4. è®­ç»ƒæ¨¡å‹
print("\n4. å¼€å§‹è®­ç»ƒ...")

history = model.fit(
    [X_train, decoder_input_train],
    decoder_target_train,
    epochs=20,
    batch_size=64,
    validation_data=([X_test, decoder_input_test], decoder_target_test),
    verbose=1
)

# 5. æ„å»ºæ¨ç†æ¨¡å‹
print("\n5. æ„å»ºæ¨ç†æ¨¡å‹...")

# ç¼–ç å™¨æ¨ç†æ¨¡å‹
encoder_model = tf.keras.Model(encoder_inputs, encoder_states, name='encoder_inference')

# è§£ç å™¨æ¨ç†æ¨¡å‹
decoder_state_input_h = tf.keras.Input(shape=(hidden_units,))
decoder_state_input_c = tf.keras.Input(shape=(hidden_units,))
decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]

decoder_single_input = tf.keras.Input(shape=(1,))
decoder_single_embedding = decoder_embedding(decoder_single_input)
decoder_single_outputs, state_h, state_c = decoder_lstm(
    decoder_single_embedding, initial_state=decoder_states_inputs
)
decoder_states = [state_h, state_c]
decoder_single_outputs = decoder_dense(decoder_single_outputs)

decoder_model = tf.keras.Model(
    [decoder_single_input] + decoder_states_inputs,
    [decoder_single_outputs] + decoder_states,
    name='decoder_inference'
)

# 6. åºåˆ—ç”Ÿæˆå‡½æ•°
def decode_sequence(input_seq):
    """
    ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹ç”Ÿæˆç›®æ ‡åºåˆ—
    """
    # ç¼–ç è¾“å…¥åºåˆ—
    states_value = encoder_model.predict(input_seq, verbose=0)
    
    # åˆå§‹åŒ–è§£ç å™¨è¾“å…¥
    target_seq = np.zeros((1, 1))
    target_seq[0, 0] = START_TOKEN
    
    # ç”Ÿæˆåºåˆ—
    decoded_sequence = []
    
    for _ in range(seq_length + 1):
        # é¢„æµ‹ä¸‹ä¸€ä¸ªè¯
        output_tokens, h, c = decoder_model.predict([target_seq] + states_value, verbose=0)
        
        # é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„è¯
        sampled_token_index = np.argmax(output_tokens[0, -1, :])
        
        # å¦‚æœé‡åˆ°ç»“æŸæ ‡è®°ï¼Œåœæ­¢ç”Ÿæˆ
        if sampled_token_index == END_TOKEN:
            break
            
        decoded_sequence.append(sampled_token_index)
        
        # æ›´æ–°è§£ç å™¨è¾“å…¥
        target_seq = np.zeros((1, 1))
        target_seq[0, 0] = sampled_token_index
        
        # æ›´æ–°çŠ¶æ€
        states_value = [h, c]
    
    return decoded_sequence

# 7. æµ‹è¯•æ¨¡å‹
print("\n6. æµ‹è¯•æ¨¡å‹...")

# æµ‹è¯•å‡ ä¸ªä¾‹å­
test_samples = 10
correct_predictions = 0

print("æµ‹è¯•ç»“æœ:")
for i in range(test_samples):
    input_sequence = X_test[i:i+1]
    target_sequence = y_test[i]
    
    # ç”Ÿæˆé¢„æµ‹åºåˆ—
    predicted_sequence = decode_sequence(input_sequence)
    
    # æ£€æŸ¥é¢„æµ‹æ˜¯å¦æ­£ç¡®
    is_correct = np.array_equal(predicted_sequence, target_sequence)
    if is_correct:
        correct_predictions += 1
    
    print(f"è¾“å…¥: {X_test[i]} -> é¢„æµ‹: {predicted_sequence} -> ç›®æ ‡: {target_sequence} {'âœ“' if is_correct else 'âœ—'}")

accuracy = correct_predictions / test_samples
print(f"\nåºåˆ—çº§å‡†ç¡®ç‡: {accuracy:.2f}")

# 8. å¯è§†åŒ–ç»“æœ
plt.figure(figsize=(12, 8))

# è®­ç»ƒè¿‡ç¨‹
plt.subplot(2, 2, 1)
plt.plot(history.history['accuracy'], label='è®­ç»ƒå‡†ç¡®ç‡')
plt.plot(history.history['val_accuracy'], label='éªŒè¯å‡†ç¡®ç‡')
plt.title('Seq2Seqæ¨¡å‹å‡†ç¡®ç‡')
plt.xlabel('è®­ç»ƒè½®æ•°')
plt.ylabel('å‡†ç¡®ç‡')
plt.legend()
plt.grid(True)

plt.subplot(2, 2, 2)
plt.plot(history.history['loss'], label='è®­ç»ƒæŸå¤±')
plt.plot(history.history['val_loss'], label='éªŒè¯æŸå¤±')
plt.title('Seq2Seqæ¨¡å‹æŸå¤±')
plt.xlabel('è®­ç»ƒè½®æ•°')
plt.ylabel('æŸå¤±å€¼')
plt.legend()
plt.grid(True)

# é¢„æµ‹å‡†ç¡®æ€§åˆ†æ
plt.subplot(2, 2, 3)
# æµ‹è¯•æ›´å¤šæ ·æœ¬
test_accuracies = []
for i in range(0, min(100, len(X_test)), 10):
    batch_correct = 0
    for j in range(10):
        if i + j < len(X_test):
            pred_seq = decode_sequence(X_test[i+j:i+j+1])
            if np.array_equal(pred_seq, y_test[i+j]):
                batch_correct += 1
    test_accuracies.append(batch_correct / 10)

plt.plot(test_accuracies, 'o-')
plt.title('æ‰¹æ¬¡é¢„æµ‹å‡†ç¡®ç‡')
plt.xlabel('æ‰¹æ¬¡')
plt.ylabel('å‡†ç¡®ç‡')
plt.grid(True)

# åºåˆ—é•¿åº¦åˆ†æ
plt.subplot(2, 2, 4)
lengths = [len(decode_sequence(X_test[i:i+1])) for i in range(min(50, len(X_test)))]
plt.hist(lengths, bins=range(seq_length + 3), alpha=0.7)
plt.title('ç”Ÿæˆåºåˆ—é•¿åº¦åˆ†å¸ƒ')
plt.xlabel('åºåˆ—é•¿åº¦')
plt.ylabel('é¢‘æ¬¡')
plt.grid(True)

plt.tight_layout()
plt.show()

# 9. ä¿å­˜æ¨¡å‹
model.save('seq2seq_model.h5')
print("\næ¨¡å‹å·²ä¿å­˜ä¸º 'seq2seq_model.h5'")

print("\n" + "=" * 60)
print("âœ… åºåˆ—åˆ°åºåˆ—æ¨¡å‹å®Œæˆï¼")
print("=" * 60)
print("ğŸ“š å­¦åˆ°çš„æ¦‚å¿µ:")
print("1. ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼šå°†è¾“å…¥åºåˆ—ç¼–ç ï¼Œç„¶åè§£ç ä¸ºè¾“å‡ºåºåˆ—")
print("2. çŠ¶æ€ä¼ é€’ï¼šç¼–ç å™¨çš„æœ€ç»ˆçŠ¶æ€ä½œä¸ºè§£ç å™¨çš„åˆå§‹çŠ¶æ€")
print("3. æ•™å¸ˆå¼ºåˆ¶ï¼šè®­ç»ƒæ—¶ä½¿ç”¨çœŸå®ç›®æ ‡ä½œä¸ºè§£ç å™¨è¾“å…¥")
print("4. æ¨ç†æ¨¡å¼ï¼šé€æ­¥ç”Ÿæˆåºåˆ—ï¼Œæ¯æ¬¡é¢„æµ‹ä¸€ä¸ªè¯")
print("5. ç‰¹æ®Šæ ‡è®°ï¼šSTARTå’ŒENDæ ‡è®°æ§åˆ¶åºåˆ—ç”Ÿæˆ")
print("\nğŸ” Seq2Seqçš„åº”ç”¨åœºæ™¯:")
print("- æœºå™¨ç¿»è¯‘ï¼šè‹±è¯­ -> ä¸­æ–‡")
print("- æ–‡æœ¬æ‘˜è¦ï¼šé•¿æ–‡æœ¬ -> æ‘˜è¦")
print("- å¯¹è¯ç³»ç»Ÿï¼šé—®é¢˜ -> å›ç­”")
print("- ä»£ç ç”Ÿæˆï¼šæè¿° -> ä»£ç ")
print("=" * 60)
